{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J11o1KYg7GRA"
   },
   "outputs": [],
   "source": [
    "! rm *.zip *.csv\n",
    "! wget bwg.140714.xyz:8000/toxic.zip \n",
    "! unzip toxic.zip \n",
    "! unzip train.csv \n",
    "! unzip test.csv\n",
    "! unzip sample_submission.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AK4X6EoD6m9M"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import os\n",
    "import timeit\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import logging\n",
    "logging.basicConfig(format='[%(asctime)s %(levelname)-8s] %(message)s', level=logging.INFO, datefmt='%m-%d %H:%M:%S')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gensim.downloader as api\n",
    "\n",
    "class DeepLearning():\n",
    "  \"\"\" A template for running CNN models\"\"\"\n",
    "  def __init__(self, max_features=100000, max_sentence_len=200, embedding_dim=100):\n",
    "    self.max_features = max_features\n",
    "    self.max_sentence_len = max_sentence_len\n",
    "    self.embedding_dim = embedding_dim # For using embedded vector\n",
    "    self.filepath=\"weights_base.best.hdf5\" # saving the best model weights \n",
    "\n",
    "  def load_data(self, train_file='train.csv', test_file='test.csv'):\n",
    "      \"\"\" A task-dependent method that will load data and do simple preprocessing,\n",
    "      @return: train_data, test_data, train_labels, test_labels\n",
    "      Load data and  \"\"\"\n",
    "      train = pd.read_csv(train_file, engine='python',\\\n",
    "          encoding='utf-8', error_bad_lines=False)\n",
    "      test = pd.read_csv(test_file, engine='python', \\\n",
    "          encoding='utf-8', error_bad_lines=False)\n",
    "      logging.info('CSV data loaded')\n",
    "      return train, test\n",
    "\n",
    "  def exploring_data(self, train):\n",
    "      '''Find patterns, informations'''\n",
    "      pass \n",
    "\n",
    "  def tokenize_text(self, text_train, text_test):\n",
    "      '''@para: max_features, the most commenly used words in data set\n",
    "      @input are vector of text\n",
    "      '''\n",
    "      tokenizer = Tokenizer(num_words=self.max_features)\n",
    "      text = pd.concat([text_train, text_test])\n",
    "      tokenizer.fit_on_texts(text)\n",
    "\n",
    "      sequence_train = tokenizer.texts_to_sequences(text_train)\n",
    "      tokenized_train = pad_sequences(sequence_train, maxlen=self.max_sentence_len)\n",
    "      logging.info('Train text tokeninzed')\n",
    "\n",
    "      sequence_test = tokenizer.texts_to_sequences(text_test)\n",
    "      tokenized_test = pad_sequences(sequence_test, maxlen=self.max_sentence_len)\n",
    "      logging.info('Test text tokeninzed')\n",
    "      return tokenized_train, tokenized_test, tokenizer\n",
    "      \n",
    "\n",
    "  def embed_glove_vector(self, word_index, model='glove-wiki-gigaword-100'):\n",
    "      glove = api.load(model) # default: wikipedia 6B tokens, uncased\n",
    "      zeros = [0] * self.embedding_dim\n",
    "      matrix = np.zeros((self.max_features, self.embedding_dim))\n",
    "      \n",
    "      for word, i in word_index.items(): \n",
    "          if i >= self.max_features or word not in glove: continue # matrix[0] is zeros, that's also why >= is here\n",
    "          matrix[i] = glove[word]\n",
    "\n",
    "      logging.info('Glove embedding vector created')\n",
    "      return matrix\n",
    "\n",
    "\n",
    "  def tfidf_vectorized(self, text_train, text_test):\n",
    "      \"\"\" Tokenize text with TfidfVectorizer()\n",
    "          Parameters such as ngram_range, max_features requires fine-tuning \n",
    "          @input: text Series, not DataFrame\n",
    "      \"\"\"\n",
    "      tv = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', \\\n",
    "                          analyzer='word', token_pattern=r'\\w{1,}',  stop_words='english', \\\n",
    "                          ngram_range=(1, 1), max_features=self.max_features)\n",
    "      # features_train = tv.fit_transform(train.comment_text)\n",
    "      # return features_train, 0\n",
    "      return (tv.fit_transform(text) for text in (text_train, test_train))\n",
    "\n",
    "\n",
    "  def build_model(self, embedding_matrix=np.zeros(0)):\n",
    "      dropout = 0.5\n",
    "      model = Sequential()\n",
    "      model.add(Embedding(self.max_features, self.embedding_dim, input_length=self.max_sentence_len))\n",
    "      model.add(Flatten())\n",
    "\n",
    "      model.add(Dense(64, activation='relu'))\n",
    "      model.add(Dropout(dropout))\n",
    "      model.add(Dense(64, activation='relu'))\n",
    "      model.add(Dropout(dropout))\n",
    "      \n",
    "      model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "      if embedding_matrix.size > 0:\n",
    "          model.layers[0].set_weights([embedding_matrix])\n",
    "          model.layers[0].trainable = False\n",
    "      logging.info(f'Model created')\n",
    "      return model\n",
    "\n",
    "  def build_lstm(self, embedding_matrix=np.zeros(0)):\n",
    "      dropout = 0.4\n",
    "      model = Sequential()\n",
    "      model.add(Embedding(self.max_features, self.embedding_dim, input_length=self.max_sentence_len))\n",
    "      model.add(Bidirectional(LSTM(64, dropout=dropout, recurrent_dropout=dropout)))\n",
    "\n",
    "      model.add(Dense(64, activation='relu'))\n",
    "      model.add(Dropout(dropout))\n",
    "      model.add(Dense(64, activation='relu'))\n",
    "      model.add(Dropout(dropout))\n",
    "            \n",
    "      model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "      if embedding_matrix.size > 0:\n",
    "          model.layers[0].set_weights([embedding_matrix])\n",
    "          model.layers[0].trainable = False\n",
    "      logging.info(f'LSTM created')\n",
    "      return model\n",
    "\n",
    "  def build_gru(self, embedding_matrix=np.zeros(0)):\n",
    "      dropout = 0.4\n",
    "      model = Sequential()\n",
    "      model.add(Embedding(self.max_features, self.embedding_dim, input_length=self.max_sentence_len))\n",
    "      model.add(Bidirectional(LSTM(64, dropout=dropout, recurrent_dropout=dropout)))\n",
    "\n",
    "      # model.add(Dense(256, activation='relu'))\n",
    "      # model.add(Dropout(dropout))\n",
    "      model.add(Dense(128, activation='relu'))\n",
    "      model.add(Dropout(dropout))\n",
    "      model.add(Dense(64, activation='relu'))\n",
    "      model.add(Dropout(dropout))\n",
    "\n",
    "      model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "      if embedding_matrix.size > 0:\n",
    "          model.layers[0].set_weights([embedding_matrix])\n",
    "          model.layers[0].trainable = False\n",
    "      logging.info(f'GRU created')\n",
    "      return model\n",
    "\n",
    "  def run(self, model, x_train, y_train):\n",
    "      checkpoint = ModelCheckpoint(self.filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "      early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "\n",
    "      model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "      X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=23)\n",
    "      history = model.fit(X_tra, y_tra, epochs=30, batch_size=128, validation_data=(X_val, y_val), \\\n",
    "                          callbacks=[checkpoint, early], verbose=1)\n",
    "      return model, history\n",
    "\n",
    "  def display_history(self, history):\n",
    "      acc = history['acc']\n",
    "      val_acc = history.history['val_acc']\n",
    "      loss = history.history['loss']\n",
    "    \n",
    "  def predict(self, y_test, labels, sub_file=\"sample_submission.csv\"):\n",
    "      res = pd.read_csv(sub_file)\n",
    "      res[labels] = y_test\n",
    "      res.to_csv('submission.csv', index=False)\n",
    "      logging.info(f\"Predictions were written to submission.csv\")\n",
    "\n",
    "  def describe_model(self, **dm):\n",
    "    for k, v in dm.items():\n",
    "      print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "time_start = timeit.default_timer()\n",
    "dl = DeepLearning(max_features=200000, max_sentence_len=300, embedding_dim=300)\n",
    "train, test = dl.load_data()\n",
    "columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "labels = train[columns].values\n",
    "\n",
    "X_train, X_test, tokenizer = dl.tokenize_text(train[\"comment_text\"].fillna(\"_na_\"), test[\"comment_text\"].fillna(\"_na_\"))\n",
    "embedding_matrix = dl.embed_glove_vector(tokenizer.word_index, 'word2vec-google-news-300')\n",
    "\n",
    "model = dl.build_lstm(embedding_matrix)\n",
    "# model = dl.build_gru()\n",
    "model, history = dl.run(model, X_train, labels)\n",
    "model.load_weights(dl.filepath)\n",
    "y_test = model.predict([X_test], batch_size=1024, verbose=1)\n",
    "dl.predict(y_test, columns)\n",
    "\n",
    "time_stop = timeit.default_timer()\n",
    "print(f'Program run for {time_stop - time_start} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSBdhCRzxOJR"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.download('submission.csv')\n",
    "strategy=\"Tokenized on all text; use glove-twitter-200 \"\n",
    "dl.describe_model(model='Bidirectional LSTM 64 Dense 128-64', max_features=dl.max_features, embed_size=dl.embedding_dim, maxlen=dl.max_sentence_len, strategy=strategy)\n",
    "\n",
    "! curl -X PUT --upload-file submission.csv ali.140714.xyz:8000\n",
    "print('DONE uploading')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EyVNqUO5DNkI"
   },
   "outputs": [],
   "source": [
    "! cp weights_base.best.hdf5 best_098012.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZa7NO_wnQEx"
   },
   "source": [
    "# Results\n",
    "* GRU_64 + Dense_64 + GloVe-Twitter-200 = 0.98166\n",
    "* GRU_64 + Dense_128 + Dense_64 no embedding \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "playground.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
