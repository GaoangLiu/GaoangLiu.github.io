---
layout: post
title: AutoEncoders
date: 2022-10-10
tags: nlp encoder
categories: nlp
author: gaoangliu
---
* content
{:toc}


# AutoEncoder

自编码器（Auto Encoder, AE） 是一种神经网络（前馈网络）模型，经过训练后可以将输入的数据压缩成一个低维的向量，然后再通过解码器将这个向量解码成原始的数据。自编码器的训练目标是使得解码器的输出与原始输入尽可能的接近，这样的训练过程可以使得自编码器学习到输入数据的内在结构，从而可以用来进行数据的**降维、去噪、异常检测**等任务。近年来，自编码器在自然语言处理领域也得到了广泛的应用，例如用于**文本压缩、文本去噪、文本生成**等任务。




<img src="https://s3.bmp.ovh/imgs/2022/10/11/a0fd38a385de34f1.png" width=678pt>

## 学习过程 

AE 学习过程使用无监督，输入样本 $$x$$ 通过编码器 $$f$$ 获得低维特征 $$z$$，最后通过解码器 $$g$$ 重构输入数据获得 $$\hat{x}$$，通过最小化重构误差来学习特征提取器。学习过程可以描述为最小化一个损失函数：

$$L(x, g(f(x)))$$

其中 $$L$$ 为损失函数，惩罚解码器输出与原始输入的差异。常用的损失函数有：
- 均方误差（MSE）：$$L(x, g(f(x))) = \frac{1}{n} \sum_{i=1}^n (x_i - \hat{x}_i)^2$$
- 交叉熵（Cross Entropy）：$$L(x, g(f(x))) = - \sum_{i=1}^n x_i \log \hat{x}_i$$
- 对数似然（Log Likelihood）：$$L(x, g(f(x))) = - \sum_{i=1}^n x_i \log \hat{x}_i - (1 - x_i) \log (1 - \hat{x}_i)$$
- KL 散度（KL Divergence）：$$L(x, g(f(x))) = \sum_{i=1}^n x_i \log \frac{x_i}{\hat{x}_i}$$
- 余弦相似度（Cosine Similarity）：$$L(x, g(f(x))) = 1 - \frac{x \cdot \hat{x}}{||x||_2 ||\hat{x}||_2}$$


# 类型 
按输入维度 $$d_i$$ 与隐藏层维度 $$d_h$$ 的关系，自编码器可以分为两种类型：
1. 欠完备自编码器（Undercomplete autoencoder）：$$d_i > d_h$$，隐藏层维度小于输入维度，隐藏层的维度通常为输入维度的 1/10 到 1/100，隐藏层的维度越小，自编码器的压缩能力越强，但是解码器的重构能力也会随之降低。
2. 过完备自编码器（Overcomplete autoencoder）：$$d_i < d_h$$，隐藏层维度大于输入维度。这种情况下，自编码器很容易学习到一个无用的恒等映射。

正则自编码器要解决的问题是，在不限制编码器、解码器层数或者编码维度的情况下，如何让自编码器学习到数据分布的有用信息。
常见的正则自编码器有以下几种：稀疏自编码器（Sparse Auto Encoder, SAE），去噪自编码器（Denoising Auto Encoder, DAE），变分自编码器（Variational Auto Encoder, VAE）。

## 稀疏自编码器

稀疏自编码器（Sparse Auto Encoder, SAE）是一种特殊的自编码器，其目的是**学习稀疏的特征**，即特征中大部分元素为 0，以便用于像分类这样的任务。训练过程中，通过添加稀疏性约束来惩罚特征 $$L(x,g(f(x))) + \Omega(x)$$。稀疏性约束可以通过 L1 正则化或 KL 散度来实现：

- L1 正则化：$$L(x, g(f(x))) = \frac{1}{n} \sum_{i=1}^n (x_i - \hat{x}_i)^2 + \lambda \sum_{i=1}^n \vert z_i \vert$$

- KL 散度：$$L(x, g(f(x))) = \frac{1}{n} \sum_{i=1}^n (x_i - \hat{x}_i)^2 + \lambda \sum_{i=1}^n z_i \log \frac{z_i}{\rho} + (1 - z_i) \log \frac{1 - z_i}{1 - \rho}$$
  - $$\rho$$ 为稀疏性约束，$$\lambda$$ 为惩罚系数。


稀疏自编码器的学习过程如下：
1. 输入样本 $$x$$ 通过编码器 $$f$$ 获得低维特征 $$z$$，最后通过解码器 $$g$$ 重构输入数据获得 $$\hat{x}$$。
2. 计算特征 $$z$$ 的稀疏度 $$s$$，稀疏度 $$s$$ 为特征 $$z$$ 中非零元素的比例。
3. 计算稀疏度 $$s$$ 与稀疏度目标 $$s^*$$ 的差值 $$s - s^*$$，并将其作为稀疏度惩罚项加入到重构误差中。
4. 最小化重构误差和稀疏度惩罚项。
5. 重复 1-4 步直到收敛。


## 去噪自编码器
除了向代价函数中添加惩罚项，我们也可以通过改变重构误差项来获得一个可学习到有用信息的自编码器。在学习过程中，我们可以通过添加噪声来改变输入数据，从而使得自编码器学习到输入数据的有用信息。这种自编码器被称为去噪自编码器（Denoising Auto Encoder, DAE）。

DAE 的学习过程可以描述为最小化：

$$L(x,g(f(\tilde{x}))) $$ 

其中 $$\tilde{x}$$ 为输入数据 $$x$$ 添加噪声后的结果。DAE 的一个动机是从数据中学习到一个高容量的编码器，但又不至于只学到一个恒等映射。


## 变分自编码器
从概率的角度来看，AE 学习到的编码器可以看作是一个隐变量的生成模型，而解码器则可以看作是一个隐变量的生成模型。在不限制自编码器的容量的情况下，给编码器及解码器足够的自由度，编码器可以将任意维度的输入编码成维度为 1 的特征，也即是可以将输出按时 1, 2, ..., $$N$$ 的顺序进行编码，同时可以保证重构误差较小。 这种情况下， AE 的隐空间可解释性较差，缺乏对输入数据的概率分布的建模能力。如果从隐空间中采样，再通过解码器重构，得到的结果与输入数据的分布相差较大。如果我们的任务是通过 AE 生成新的数据，那么这样的 AE 就不太适合。

[变分自编码器（Variational autoencoder, VAE）](https://arxiv.org/abs/1312.6114)，是 AE 的一种变体，在文本生成、图像风格迁移、合成音乐等诸多任务中有显著的效果。与常规 AE 的区别在于，VAE 将输入映射到隐空间的一个概率分布，而不是一个点。通过对隐空间的分布进行正则化，以确保隐空间具有良好的属性，使得可以通过在隐空间中采样生成一些新的数据。

隐空间的正则性有两层含义：
1. 连续性（continuity），隐空间中的两个相邻点解码后不应呈现两个完全不同的内容
2. 完整性（completeness），针对给定的分布，从隐空间采样的点在解码后应提供“有意义”的内容

VAE 模型的训练流程为：
- 将输入编码为在隐空间上的分布
- 从该分布中采样隐空间中的一个点；
- 对采样点进行解码并计算出重建误差；
- 重建误差通过网络反向传播。

VAE 与普通 AE 在流程上的差异如下图所示：

<img src="https://miro.medium.com/max/4800/1*ejNnusxYrn1NRDZf4Kg2lw@2x.png" width=678pt>

### 编码成什么样的分布 ？
那么上面提取的 DAE 及 SAE 能否胜任数据生成的任务呢？答案是不能。DAE 与 SAE 都是通过添加噪声或稀疏度惩罚项来限制 AE 的容量，从而使得 AE 学习到的编码器与解码器的映射关系更加复杂，但是这种方法并不能保证 AE 编码后的隐空间具备以上正则性。 

同样，仅将输入编码成分布也是不够的。如果没有对隐空间的概率分布进行正则，为了最小化重建误差，模型在学习过程逐步"忽略"要返回多个分布的要求。比如，编码器将输入映射到一批方差较小的分布（即一个分布可能的取值点集中在一起）或一批平均值差异较大的分布（数据点分散的较开）。在这两种情况下，连续性和完整性都无法同时满足。

为避免这种现象，一种方法是对**分布的协方差矩阵及平均值进行正则**。在实践中，这种正则化是通过强制要求分布接近标准正态分布（居中和缩小）来实现的。这样，我们就要求协方差矩阵接近于单位矩阵，以防止准时分布，并要求平均值接近于0，以防止编码后的分布彼此相差太远。

> 注：为什么正则协方差矩阵？因为协方差刻画了两个变量**线性相关性**的强度以及这些变量的尺度。$$Cov(f(x), g(x)) = E[(f(x) - E[f(x)]) (g(x) - E[g(x)])]$$。协方差的绝对值越大，说明变量值变化很大，并且它们同时距离各自的均值很远。如果协方差是正的，那么两个变量都倾向于同时取得较大的值。如果协方差是负的，那么一个变量倾向于取得较大的值，而另一个变量倾向于取得较小的值。如果协方差是 0，那么两个变量之间没有线性关系。同一性矩阵是一个对角矩阵，对角线上的元素都是 1，其余元素都是 0。因此，协方差矩阵接近同一性矩阵，就是说协方差矩阵的对角线上的元素都接近 1，其余元素都接近 0。这样，我们就要求协方差矩阵接近于同一性，以防止准时分布。


### 为什么要使用 KL 散度 ？
KL 散度是一种衡量两个分布之间差异的度量。KL 散度的定义为：
$$KL(P \parallel Q) = \sum_{i=1}^n P(x_i) \log \frac{P(x_i)}{Q(x_i)}$$


### 参考 
- [Understanding Variational autoencoders (VAEs)](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)


# Go deeper
- VAE， GAN 的关系，对比。 


# 参考 
- [autoencoders implemetation, Tensorflow](https://www.tensorflow.org/tutorials/generative/autoencoder)
- [What is Variational autoencoder, JAAN ALTOSAAR](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)
- [Variational autoencoder, wikipedia](https://en.wikipedia.org/wiki/Variational_autoencoder)
- [Auto-Encoding Variational Bayes, arxiv](https://arxiv.org/abs/1312.6114)