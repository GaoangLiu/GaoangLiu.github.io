---
layout: post
title: Gradient descent
date: 2023-02-01
tags: nlp gradient
categories: nlp
author: gaonagliu
---
* content
{:toc}


由于引入了非线性激活函数，神经网络的非线性导致了损失函数的非凸性，无法直接求解损失函数的最优解。因此，神经网络的训练过程通常采用**基于梯度**的优化算法，通过迭代的方式逐渐优化损失函数，从而得到模型的最优参数。




# 反向传播 
Back Propagation，BP。BP 经常被理解成神经网络整个的训练方法，但 BP 只是**一种高效计算神经网络参数梯度的方法**。它利用了**链式法则**，将参数梯度的计算拆分为多个小的计算单元，从而降低了计算复杂度。

神经网络的训练过程中，前向传播和反向传播交替进行。前向传播通过训练数据和权重参数计算输出结果；反向传播通过导数链式法则计算损失函数对各参数的梯度，并根据梯度进行参数的更新。

## 反向传播算法
BP 算法中一个重要的概念是链式法则，它是一种计算复合函数导数的方法。对于复合函数 $$f(g(x))$$，链式法则表明，它的导数可以通过将外层函数 $$f(x)$$ 关于内层函数 $$g(x)$$ 的导数乘以内层函数 $$g(x)$$ 关于自变量 $$x$$ 的导数来求解。

在反向传播算法中，有多种常用的参数更新方法，以下是其中几种常见的方法及其优缺点：

1. 梯度下降法（Gradient Descent）：
    - 优点：简单易实现，容易收敛。
    - 缺点：可能收敛到局部最优点，参数更新速度较慢，在目标函数有多个局部最小点时效果可能较差。
2. 随机梯度下降法（Stochastic Gradient Descent, SGD）：
    - 优点：更新速度快，对于大规模数据集具有高效性。
    - 缺点：可能会在训练过程中出现波动，收敛过程不稳定，容易陷入局部最优。
3. 小批量梯度下降法（Mini-Batch Gradient Descent）：
    - 优点：兼具了梯度下降法和随机梯度下降法的优点，更稳定且计算效率高。
    - 缺点：需要对小批量大小进行合理选择，过大会导致计算复杂度增加，过小可能会降低优化的效果。
4. 动量优化算法（Momentum Optimization）：
    - 优点：加入动量项，能够帮助跳过局部最小点，加速收敛速度。
    - 缺点：需要调节动量超参数，不易调节。
5. 自适应学习率优化算法（Adaptive Learning Rate Optimization）：
    - 优点：自动调整学习率，根据参数梯度进行动态更新，能够在不同参数及任务中获得更好的效果。
    - 缺点：计算复杂度较高，需要更多的计算资源。 总体而言，选择合适的参数更新方法取决于不同的问题和数据集。
    

### Momentum

GD 只考虑了当前步骤的梯度，而没有考虑到之前步骤的梯度。权重的更新学习只取决于学习率及当前的梯度。存在的问题
1. 如果损失函数的梯度方向在某一维度上一直为 0，那么这一维度上的权重就不会更新，导致模型无法学习到这一维度的特征。

$$\Delta w_t = -\eta \nabla L(w_t)$$

Momentum 将之前的梯度也考虑进来，从而可以在一定程度上解决上述问题。

$$\Delta w_t = -\eta \nabla L(w_t) + \alpha \Delta w_{t-1}$$

其中，$$\alpha$$ 是 momentum 的超参数，通常取 0.9。

### Nesterov accelerated gradient

NAG 算法在动量梯度下降的基础上进行了改进，增加了一项校正项。具体来说，NAG 算法首先根据当前参数的动量方向进行一次更新，然后再根据校正后的位置计算损失函数的梯度，最后再次进行参数的更新。

通俗的说，NAG 知道参数 $$w_t$$ 下一步会走 $$\alpha \Delta w_{t-1}$$，那先走过去，然后计算梯度，再走一步。

$$\Delta w_t = -\eta \nabla L(w_t + \alpha \Delta w_{t-1}) + \alpha \Delta w_{t-1}$$

### AdaGrad

AdaGrad 是一种自适应学习率的方法，旨在解决传统梯度下降算法中学习率选择困难的问题。 AdaGrad 算法的核心思想是根据参数的梯度历史信息来调整学习率，使得参数在更新过程中能够更加适应不同特征的梯度变化。

$$\Delta w_t = -\frac{\eta}{\sqrt{G_t + \epsilon}} \nabla L(w_t)$$

其中，$$G_t$$ 是之前的梯度平方和，$$\epsilon$$ 是一个很小的数，防止分母为 0。

优点：能够自动调整学习率，使得在参数空间中较大梯度的方向上学习率变小，较小梯度的方向上学习率变大，从而提高算法的收敛性能。
缺点：随着训练的进行，累积平方梯度会趋近于无限增大，导致学习率过快衰减，可能导致算法提前停止收敛。

代码实现：

### RMSProp

RMSProp(Root Mean Square Propagation) 算法是对 AdaGrad 算法的改进，它通过引入一个衰减系数 $$\gamma$$ 来控制历史信息的获取程度，从而解决 AdaGrad 算法中学习率过快衰减的问题。

$$\Delta w_t = -\frac{\eta}{\sqrt{G_t + \epsilon}} \nabla L(w_t)$$

### Adam

Adam(Adaptive Moment Estimation) 算法是一种结合了 Momentum 和 RMSProp 思想的自适应学习率优化算法。它不仅利用了梯度的一阶矩估计（即梯度的均值），还利用了梯度的二阶矩估计（即梯度的未中心化的方差）。

$$\Delta w_t = -\frac{\eta}{\sqrt{\hat{G}_t + \epsilon}} \hat{m}_t$$

其中，$$\hat{m}_t$$ 是梯度的一阶矩估计，$$\hat{G}_t$$ 是梯度的二阶矩估计。

优点：结合了 Momentum 和 RMSProp 的优点，不仅能够自适应调整学习率，还能够在参数空间中更加平滑地更新参数。

缺点：需要调节更多的超参数，计算复杂度较高。


