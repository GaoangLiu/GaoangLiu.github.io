---
layout: post
title: Self Supervised Learning
date: 2023-05-01
tags: 
categories: ssl,_np,_deep_learning
author: berrysleaf
---
* content
{:toc}


> 本文是对自监督学习的一些总结，包括自监督学习的定义、发展历史、应用场景、方法、评价指标等。



> 来源： https://arxiv.org/pdf/2304.12210.pdf

被称为“人工智能领域的暗物质”的自监督学习（Self-supervised learning），是推动机器学习发展的一条有前途的道路。相比于有标注数据限制的监督学习，自监督方法可以从大量无标注数据中进行学习 [Chen et al., 2020b, Misra and Maaten, 2020]。自监督学习在自然语言处理领域中的成功，推动了深度学习的发展，从自动化机器翻译到基于网络规模的未标注文本体裁的大型语言模型的训练 [Brown et al., 2020, Popel et al., 2020]。在计算机视觉领域，自监督学习通过模型（如SEER）训练了10亿张图像的数据规模，推动了数据规模的新突破[Goyal et al., 2021]。自监督学习的计算机视觉方法能够与有标注数据训练的模型相匹配，甚至在ImageNet等高度竞争的基准测试上有时能够超过这些模型[Tomasev et al., 2022, He et al., 2020a, Deng et al., 2009]，同时在其他模态（如视频、音频和时间序列等）上也得到了成功的应用 [Wickstrøm et al., 2022, Liu et al., 2022a, Schiappa et al., 2022a]。

自监督学习定义了一个基于未标注输入的前提任务来生成描述性和易懂的表示形式 [Hastie et al., 2009, Goodfellow et al., 2016]。在自然语言处理中，一种常见的自监督目标是用掩码隐藏文本中的单词并预测周围的单词(MLM)。这个预测单词上下文的方式鼓励模型在没有标注数据的情况下捕获文本中单词之间的关系。训练后的模型表示可以用于一系列的下游任务，比如跨语言翻译、摘要或生成文本等。在CV领域，也有类似的做法，例如MAE或BYOL模型学习预测图像或表示中掩码的块[Grill et al., 2020, He et al., 2022]。其他自监督目标则鼓励将通过添加颜色或裁剪等方式形成的同一图像的两个视图映射到相似的表示形式上。

使用大量无标注数据进行训练，优势在哪里？

SL通过在标注数据上针对特定任务进行训练，而SSL则学习了可适用于许多任务的**通用表示**。在标注成本昂贵或特定任务事先无法知道的领域（如医学）中，SSL特别有用[Krishnan et al., 2022, Ciga et al., 2022]。同时还有证据表明，相比于SL，SSL模型可以学习更具鲁棒性的表示形式，对抗示例、标签损坏和输入扰动的影响更小，并且更加公平[Hendrycks et al., 2019, Goyal et al., 2022]。


