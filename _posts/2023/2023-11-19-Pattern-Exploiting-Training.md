---
layout: post
title: Pattern Exploiting Training
date: 2023-11-19
tags: peft
categories: nlp
author: berrysleaf
---
* content
{:toc}


<!-- excerpt -->
[Pattern Exploiting Training(PET)](https://arxiv.org/abs/2001.07676，[论文直达](https://host.ddot.cc/2001.07676.pdf)) 是一种少样本半监督训练方法，通过添加任务描述，将输入重构成填空风格（cloze-style）的语句。使用少量样本微调 MLM 模型，然后集成模型进行软标注，最后通过知识蒸馏训练最终的模型。 在样本量不到 1000 条的情况下，PET 在 Yelp, AG's news 等任务上效果超过了有监督训练。





# A bird's eye view

本质上是一种少样本学习方法。得益于 GPT、BERT 等 PLMs 强大的表达及推理能力，NLP 中一些任务可以直接使用 GPT、BERT 进行 zero-shot 推理，比如分类、文本蕴含等。不同任务的目标不一样，使用同一个模型进行推理时，需要引入一些“额外的信息”让 PLM 知道它要怎么处理这个任务，这个“额外的信息”就是 prompt。

Prompt 可以是任务表述，如“判断下面评论的情感任性：「用户评论」”，也可以是一些模板，如“「用户评论」。这条评论是 [MASK] 的。”。Zero-shot 推理时，人工设计合适的 prompt, 然后把 prompt 与输入拼接起来，输入到模型中，就可以得到任务的输出。这种做法的思路是通过**对输入进行改造，挖掘语言模型的潜力，获得任务相关的知识**。GPT-2 出来之后，就有工作结合 zero-shot 将文本分类任务转换成问答任务，例如：[Zero-shot Text Classification With Generative Language Models](https://arxiv.org/pdf/1912.10165.pdf)。

这个思路要考虑的问题有：
1. **prompt 设计**：如何设计合适的 prompt，激发模型的潜能。
2. **输出映射**：模型输出的未必是任务要求的最终格式，例如如何把 GPT-2 生成的 "a mixture of utterly pointless painfully boring scenes" 跟扎导新作 [《Rebel Moon: Part One - A Child of Fire (2023)》](https://www.imdb.com/title/tt14998742/reviews/?ref_=ttrt_ql_2) 的评论极性判断任务对应起来。
3. **低资源下模型优化**：如果存在少量标注数据，如何微调模型，使得模型能够学到更多任务相关的知识。

PET 主要针对低资源的场景，它对应地的解决方案是：
1. **模板填空**：将输入转换成填空风格的语句。
2. **映射函数**：限定输出值域，然后映射到任务标签上。这个工作是考虑的任务都是分类（Yelp reviews, AG's News, Yahoo Questions）或者语义蕴含任务，所以简单的映射就可以。 
3. **模型集成 + 数据增强**：训练多个弱模型，集成打标，然后在伪标签数据集上训练一个强模型。


# 形式化定义

PET 定义了两个概念：**pattern** 和 **verbalizer**。Pattern 用于实现上面的“模板填空”，verbalizer 即“映射函数”，这个术语过于罕见，在没有看文献之前，这个概念一直困惑着笔者，特别是讨论时听别人一直 `verbalizer` 的时候，总觉得这个词难以捉摸、深不可测，背后一定有一套深邃复杂的逻辑。自己看了文献后，发现就是一个普通映射函数而已。其实原作者直接叫 `mapping` 也没什么问题，这样读者理解交流起来也清晰方便，可能是 `verbalizer` 这个词更容易唬人吧。 

<figure style="text-align: center;">
    <img src="https://host.ddot.cc/tmp_verbalizer_20231223_1404.png" width=667pt>
    <figcaption style="text-align:center"> Claude 关于 verbalizer 的解释 </figcaption>
</figure>


形式化地，令 $$M$$ 表示一个语言模型，$$V$$ 表示词汇表，$$\mathcal{L}$$ 表示分类任务的标签集合。令 $$s_i \in \Omega = V^*$$ 表示一条序列，这个序列可以是一条短语或者一条句子。那么：

1. *pattern*，$$P: \Omega^k \rightarrow \Omega$$ 是将输入 $$x=(s_1, ..., s_n) \in \Omega^n$$ 映射到一条序列的函数。即 $$P$$ 将样本中的所有序列拼接起来，得到一条新的序列。例如，对于文本相似度任务 $$T$$，$$x$$ 形如 $$x=(s_1, s_2)$$，可以定义 $$P(x) = s_1 \text{[M]} s_2$$。电影评论情感分类任务 $$T$$，$$x$$ 就是一条评论，可以定义 $$P(x) = x \text{. All in all, the film was [M]. }$$。
2. *verbalizer*， $$v:\mathcal{L} \rightarrow V$$，将每一个标签映射到词汇表中的一个词。例如，对于情感分类任务，可以定义两个 verbalizer，分别是 $$v_+$$ 和 $$v_-$$，将 positive 和 negative 映射到词汇表中的一个词。 

## Yelp 评论示例 
Pattern 是对原始输入进行格式转换，适配下游模型，verbalizer 是将下游模型的输出映射到任务标签上。以 Yelp 评论打分为例，给定一条评论 $$x$$，可能的 patterns 有：
1. $$P_1(x) = \text{It was [M]. } x$$; 
2. $$P_1(x) = x \text{. All in all, it was [M]. }$$; 

其中 $$\text{[M]} \in V$$ 表示一个 mask 词。对应的 verbalizer 为 $$v: [1,2,3,4,5] \rightarrow \{ \text{great, good, okay, bad, terrible} \}$$.

考虑到模型生成一个可能的词后，我们把这个词映射到标签值，才算把任务完成。那么这里有一个问题，<span style="color:#E9AB17">为什么 verbalizer $$v:\mathcal{L} \rightarrow V$$ 是从标签值映射到词汇表，而不是反过来呢</span>？

因为 $$v$$ 是一个单射，且 $$\lvert \mathcal{L} \rvert$$ 是有限的，那么值域也是有限的，参考上面的 Yelp 例子，其中 $$\mathcal{L}=[1,2,3,4,5]$$，对应的 $$v(\mathcal{L})$$ 也只有 5 个可能的值 "great, good, okay, bad, terrible"。在训练与推理时，PET 只关注这一部分词汇，模型做填空题生成的所有可能的词里面，只保留这 5 个词，它们概率分布可以通过原始概率归一化获得，下面的训练中其实就是这么做的。这样做的好处是，训练时，给定标签值，e.g., 5，我们就知道应该给模型什么样的监督标签，e.g., $$v(5)=\text{great}$$。

回到刚才的问题，其实 verbalizer 反过来设计也是可以的，即 $$v: V \rightarrow \mathcal{L}$$，那这个时候 $$v$$ 可以是一个满射，例如，除了把 "great" 映射到 5 分之外，还可以把 "amazing, awesome, mindblowing" 等词也映射到 5。当然，前提是，few-shot learning 后，模型能输出这些词。

为简单起见，假设这 5 个词跟打分仍然一一对应，即 $$v(\text{great})=5, ..., v(\text{terrible})=1, v(\text{OTHERS})=0$$，最后一项表示除上述 5 个词之外的所有词都映射到 0 。那训练时，给定输入 $$x$$，我们用 pattern 转化一下得到 $$P(x)$$，它对应的真实标签 $$l$$，那么 $$l$$ 的得分可以记为： 

$$s_p(l) = M(w \lvert P(x)) \text{ s.t. } v(w) = l$$

这其实跟下面的公式 (E-score) 没什么本质区别，更多的是形式化表示上的差异，要做的事情还是一样的。唯一有差异的地方在于，一个标签必须要对应多个可能的词汇时，$$v: V \rightarrow \mathcal{L}$$ 的形式化表示就很方便了：

$$s_p(l) = \sum_{w \in V} M(w \lvert P(x)) \text{ s.t. } v(w) = l$$

## 训练 
Pattern 实际上把输入序列转成 MLM 的输入，MLM 对这个序列进行预测，得到一个概率分布，用 $$M(w | z), w \in V, z \in \Omega$$ 表示。 给定  $$p=(P, v)$$，定义 $$l \in \mathcal{L}$$ 的得分为：

$$\begin{equation}
\tag{E-score}
s_p(l) = M(v(l) \lvert P(x))
\end{equation}$$

通过 softmax 函数，可以得到一个概率分布：

$$q_p(l) = \frac{\exp(s_p(l))}{\sum_{l' \in \mathcal{L}} \exp(s_p(l'))}$$


# PVP 集成 
PVP 的选择对效果有直接的影响，因为是低资源场景，训练数据比较少，验证数据就更少了。因此缺乏一定量的数据来验证一个 PVP 的效果，也就很难判断哪一个 PVP 更合适。针对这个问题，作者通过**模型集成**与**模型蒸馏**的思想进行训练。集成这一步比较好理解，“三个臭皮匠，顶个诸葛亮”，单个 PVP 的效果不好确定，那就多个 PVP 一起训练，结果最起码稳定一些，但为什么还要加上蒸馏这一步呢？

因为不做蒸馏的话，这么多模型在推理时都需要走一遍，这样的话，推理时间会变长，这个方案就没有什么优势。 蒸馏的目的是将这些模型的知识注入到一个模型上，这样推理时只需要一个模型，推理时间就会大大缩短。说白了，就是速度与效果的权衡。

1. **微调**。首先使用多个不同 PVP 设定的 PLM 微调，得到多个「PVP 微调模型」。
2. **集成**。然后将这些模型集成，得到一个「PVP 集成模型」。集成的方法是将所有模型的输出相加，然后归一化，得到一个概率分布。
3. **蒸馏**。将知识蒸馏到输出模型 $$C$$。即使用「PVP 集成模型」对无标注数据集进行打标，得到一个伪标签数据集，然后在这个数据集上训练 $$C$$。

形式化的，根据直觉定义一批 PVPs $$\mathcal{P}$$，对每一个  $$p\in \mathcal{P}$$ 都微调一个 $$M_p$$。然后集成这样模型 $$\mathcal{M} = \{ M_p \lvert p \in \mathcal{P} \}$$ 对无标注样本集 $$\mathcal{D}$$ 进行打标:

$$s_\mathcal{M}(l |x) = \frac{1}{Z} \sum_{p \in \mathcal{P}} w(p) \cdot s_p(l | x)$$

其中，$$Z= \sum w(p)$$，$$w(p)$$ 表示每一对 $$(P, v)$$ 的权重。

权重的设计有很多种方案，简直暴力一点的，可以将所有权重都调成相等的值，即 $$w(p) = 1$$，或者引入先验知识赋予不同的值。 文中给定的另一种方案是 $$w(p) = \text{precision}_{p, \mathcal{T}}$$，即使用 $$p$$ 在训练集 $$\mathcal{T}$$ 的精度。在论文实验（文中 Table 4）里，两种权重方案的效果没有明显差异 ($$\lvert \mathcal{T} \rvert = 10$$)。 

<figure style="text-align:center">
    <img src="https://image.ddot.cc/202311/pet_20231129_1423.png" width=678pt>
    <figcaption style="text-align: center;"> PET schematic representation </figcaption>
</figure>



# iPET
除子比较直观的集成+蒸馏方案外，研究人员还设计验证了迭代版本的 PET，即通过**训练——模型标注——再训练——再标注**的迭代 $$k$$ 次获得最终数据集。 

上面的方案中有一些 pattern 效果比较差，因此软标签里包含了很多错误的标注。iPET 相当于把上面“一步到位”的方法拆成了若干步去迭代。按理说，“一步到位”存在的问题，iPET 也存在，那 iPET 效果却比“一步到位”好，背后的原因是什么呢？iPET 就不存在大量错标的情况吗？

研究人员给出的直觉是，**即使不微调模型，预测值置信度高的样本通常结果也都是正确的**，这个直觉来源于
 17 年的工作[On Calibration of Modern Neural Networks](https://proceedings.mlr.press/v70/guo17a/guo17a.pdf)。在迭代过程中，iPET 都是选一些置信度高的样本作为软标签，用于训练下一阶段的模型。 因些，总体的直觉是，**每一步都谨慎一点，少错一点，最终的结果就会好一点**。


通过实验验证，当数据集较小，比如只有几十个样本时，iPET 能带来几个点的效果提升。为了验证迭代的有效性，作者还设计了一个实验， 在 Yelp、AG News 等 4 个数据集上使用 iPET 进行 zero-shot 学习，总迭代轮次等于 4。对 AG News、Yahoo 任务还设计了跳过第二、三次迭代，即第一次迭代后直接打标 $$d^3 \cdot \lvert \mathcal{T}_1 \rvert$$ 个数据供 $$\mathcal{M}_4$$ 训练。 实验有如下结果：

1. 随着迭代次数增加，模型效果也逐渐提升； 
2. 跳过中间迭代过程，一次性打标相同数量样本并训练的效果**弱于逐步迭代**的效果。 


<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/ipet_generation_result_20231201_1401.png" width=678pt>
    <figcaption style="text-align: center;"> iPET schematic representation </figcaption>
</figure>


训练时的几点细节：
1. 每次标注时，不是所有模型一起标，而是**随机**选择模型的一个子集进行标注； 
2. 在每一次迭代中，数据等比缩放。 通过随机采样，保证新标注数据中各标签的比例都与原始数据中的比例保持一致，即 $$\lvert \mathcal{T}_i \rvert = d \cdot \lvert \mathcal{T}_{i-1} \rvert $$，其中 $$d$$ 是缩放比例。在原文实验中，$$d$$ 设置为 5，迭代终止条件是每个模型都最终在 1000 条样本上训练，即轮次 $$k = \lceil \log_d(1000 / \lvert \mathcal{T} \rvert)$$；
3. 在整个未标注数据集 $$\mathcal{D}$$ 上都进行打标，得到 $$\mathcal{T}_\mathcal{N} = \{ (x, \text{argmax}_{\substack{l \in \mathcal{L}}} s_\mathcal{N}(l \lvert x)) \lvert x \in \mathcal{D}\}$$，在下一步训练使用时，从所有伪标数据集中抽取一部分。一对样本 $$(x, y)$$ 被抽取的概率正比于 $$s_\mathcal{N}(l \lvert x)$$。


# 效果 
## 纵向对比  
PET 对比 supervised 方案。 整体上而言，训练集 $$\mathcal{T}$$ 越小，PET 提升越大。以分类任务 Yelp, AG News 例，iPET 在 $$\lvert \mathcal{T} \rvert = 10$$ 上的提升分别为 0.365 和 0.642，而在 $$\lvert \mathcal{T} \rvert = 100$$ 上的提升分别为 0.099 和 0.036。因此一个粗糙的结论：**对于分类任务，如果在几百条标数据上效果不好，那个效果基本上也接近半监督学习的上限了**。
差异比较大的是 MNLI 数据集，在 $$\lvert \mathcal{T} \rvert = 1000$$ 时，iPET 对比 supervised 仍然有 10 个点的提升。

## 横向对比
对比两个基于数据增强的方案 UDA、MixText， PET 在数据量比较小（小于 50）时都表现出明显的优势。  



<figure style="text-align:center">
    <img src="https://image.ddot.cc/202311/ipet_vs_supervised.png" width=678pt>
    <figcaption style="text-align: center;"> iPET v.s. Supervised </figcaption>
</figure>


# 扩展阅读 
- Prompt-Tuning：深度解读一种新的微调范式, [https://zhuanlan.zhihu.com/p/618871247](https://zhuanlan.zhihu.com/p/618871247)

