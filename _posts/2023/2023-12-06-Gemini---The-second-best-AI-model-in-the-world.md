---
layout: post
title: Gemini - The second best AI model in the world
date: 2023-12-06
tags: gemini llm deepmind
categories: nlp
author: gaoangliu
---
* content
{:toc}


è‡ªäº”æœˆä»½ Google åœ¨ [I/O å¤§ä¼š](https://blog.google/technology/ai/google-io-2023-keynote-sundar-pichai/#ai-responsibility) ä¸Šå®£å¸ƒ Gemini æ¨¡å‹å¼€å§‹è®­ç»ƒä»¥æ¥ï¼Œå†æ—¶åŠå¹´æœ‰ä½™ï¼ŒGoogle Gemini ç»ˆäºéš†é‡ç™»åœºã€‚ä¸æ­¤åŒæ—¶ï¼ŒOpenAI çš„  [ChatGPT](https://chat.openai.com) å‘å¸ƒåˆšå¥½ä¸€å‘¨å¹´ã€‚





# Google is so back

è‡ª ChatGPT é—®ä¸–ä»¥æ¥ï¼ŒåŒ…æ‹¬ç¬”è€…åœ¨å†…çš„å¾ˆå¤šç”¨æˆ·åœ¨æœç´¢ä¹ æƒ¯ä¸Šéƒ½ç»å†äº†ä¸€åœºç¿»å¤©è¦†åœ°çš„å˜é©ã€‚æœ‰ç¼–ç¨‹ã€è®¾è®¡ã€æ–¹æ¡ˆã€ç”Ÿæ´»åŠå“²å­¦é—®é¢˜éœ€è¦è§£ç­”ï¼Ÿç›´æ¥æŠ›ç»™ ChatGPTï¼Œåªè¦ prompt å†™å¾—å¾—å½“ï¼Œæ²¡æœ‰ ChatGPT è§£å†³ä¸äº†çš„é—®é¢˜ï¼Œå½“ç„¶ï¼Œæ¨¡å‹â€œå¹»è§‰â€åˆæ˜¯å¦å¤–ä¸€å›äº‹äº†ã€‚è¿™ç§å˜åŒ–å¯¹ Google æœç´¢å¯è°“ä¸€è®°é‡æ‹³ï¼Œç›´æ¥å¯¼è‡´ä¸€æ®µæ—¶é—´å†… Google çš„æœç´¢ä»½é¢å¤§å¹…ä¸‹é™ã€‚ChatGPT å¯¹ Google æœç´¢çš„å¨èƒä¹‹å¤§ï¼Œæ®è¯´è¿åˆ›å§‹äººéƒ½å¼€å§‹äº²è‡ªä¸‹åœºæ LLMï¼ˆ[Back At Google Again, Cofounder Sergey Brin Just Filed His First Code Request In Years â€”â€” Forbes](https://dlj.one/zprxug)ï¼‰ã€‚

åœ¨ AI é¢†åŸŸï¼ŒGoogle å…¶å®ä¸€ç›´å¤„äºé¢†å…ˆåœ°ä½ï¼Œæ¯•ç«Ÿæ——ä¸‹æœ‰ Google Brain å’Œ DeepMind ä¸¤å¤§é¡¶çº§ AI ç ”ç©¶æœºæ„ï¼ŒDeepMind å‡ºå“çš„ AlphaGo ä¹Ÿæ˜¯å½“å¹´é£å¤´æ— ä¸¤ã€å®¶å–»æˆ·æ™“çš„æ˜æ˜Ÿ AI ä½œå“ã€‚åœ¨ NLP é¢†åŸŸï¼ŒGoogle ä¹Ÿé¢‡æœ‰å»ºæ ‘ï¼Œç‡å…ˆæå‡ºäº† transformer ç»“æ„ï¼Œä¹Ÿå…ˆåå‘å¸ƒäº† BERTã€T5ã€ALBERTã€Switch Transformer ç­‰å¤šä¸ªçŸ¥åæ¨¡å‹ã€‚ä½†æ˜¯ï¼Œå’Œ OpenAI çš„ GPT ç³»åˆ—å¯¹æ¯”ï¼ŒGoogle åœ¨èŠå¤©æœºå™¨äººè¿™å—æœ‰ç‚¹è·Ÿä¸ä¸ŠèŠ‚å¥ï¼Œåœ¨ ChatGPT é—®ä¸–ä¸‰ä¸ªæœˆä¹‹åï¼ŒGoogle æ‰åŒ†å¿™èµ¶ [Bard](https://bard.google.com) ä¸Šæ¶ã€‚Bard åšä¸º Google çš„ç¬¬ä¸€æ¬¾ AI èŠå¤©æœºå™¨äººï¼Œè¢«å¹¿å¤§ç”¨æˆ·å¯„äºˆäº†åšæœ›ï¼Œä½†å¥ˆä½• Bard æ•ˆæœç€å®å¤ªå·®ï¼Œè¿ä¸ªç®€å•çš„åŠ å‡æ³•éƒ½æä¸å®šï¼Œè¯­è¨€èƒ½åŠ›æ›´æ˜¯ä¸€è¨€éš¾å°½ï¼Œæ—©æœŸçš„ç‰ˆæœ¬ç”šè‡³åªæ”¯æŒè‹±æ–‡ï¼Œå¯¹æ¯” ChatGPT ç®€ç›´æ˜¯äº‘æ³¥ä¹‹åˆ«ï¼Œä¹Ÿéš¾æ€ªæœ‰äººè°ƒä¾ƒç§° [â€œBard is a jokeâ€](https://twitter.com/high_byte/status/1639596716339896322)ã€‚ä¸ºäº†èµ¶è¶… OpenAIï¼ŒGoogle ä¸æƒœå°†ä¸¤ä¸ª AI ç ”ç©¶æœºæ„åˆå¹¶ï¼Œæˆç«‹äº† Google DeepMindã€‚åœ¨ä»Šå¹´ 5 æœˆä»½çš„ Google I/O å¤§ä¼šä¸Šï¼ŒGoogle å®£å¸ƒæ–°çš„ Google DeepMind å®éªŒå®¤å·²å¼€å§‹å¼€å‘ Geminiã€‚

ç»è¿‡åŠå¹´çš„æ²‰å¯‚ï¼ŒGoogle DeepMind ç»ˆäºæäº†ä¸€ä¸ªå¤§æ–°é—»ï¼Œå‘å¸ƒäº† [Gemini 1.0](https://deepmind.google/technologies/gemini/) å¤šæ¨¡æ€ AI è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶æ”¯æŒæ–‡å­—ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šç§è¾“å…¥æ•°æ®çš„å¤„ç†ã€‚æ ¹æ® CEO Sundar Pichai åœ¨ [twitter](https://twitter.com/sundarpichai/status/1732433036929589301) ä¸Šå‘å¸ƒçš„æ•ˆæœå±•ç¤ºè§†é¢‘ä¸­å¯ä»¥çœ‹åˆ°ï¼ŒGemini åœ¨éŸ³é¢‘ã€å›¾ç‰‡è¯†åˆ«ã€æ–‡å­—ç†è§£ã€æ¨ç†æ–¹é¢éƒ½è¡¨ç°å‡ºæƒŠäººçš„èƒ½åŠ›ã€‚æ›´ä»¤äººæŒ¯å¥‹çš„æ˜¯ï¼ŒGemini åœ¨ä¸€ç³»åˆ—å¼€æºæ•°æ®é›†ä»»åŠ¡çš„è¯„æµ‹ä¸­éƒ½åˆ·æ–°äº† SoTAï¼Œæ•ˆæœå ªç§°å‡ºè‰²ï¼Œå¯ä»¥è¯´æ˜¯ç›®å‰æœ€å¼ºç»¼åˆ AI è¯­è¨€æ¨¡å‹äº†ã€‚

ä¸€ç»å‘å¸ƒï¼ŒTwitterã€YouTubeã€å¾®ä¿¡å…¬ä¼—å·ã€æœ‹å‹åœˆçº·çº·æ¶Œç°å‡ºäº† `Gemini`ã€`SoTA`ã€`å¤šæ¨¡æ€` çš„æ¶ˆæ¯ï¼Œå†ç°å½“åˆ ChatGPT åˆšå¤§ç«æ—¶çš„çƒ­é—¹åœºé¢ã€‚ç»™äººçš„æ„Ÿè§‰æ˜¯ï¼ŒGoogle ç»ˆäºä¸€é›ªå‰è€»ï¼ŒæˆåŠŸç™»ä¸Šä¸€ç»Ÿ AI é¢†åŸŸçš„éœ¸ä¸»åœ°ä½ã€‚

<figure style="text-align: center;">
    <img src="https://image.ddot.cc/202312/gemini_openai_20231207_2247.png" width=445pt>
    <figcaption>Google is so back</figcaption>
</figure>

## ç‰ˆæœ¬ä¸æ•ˆæœ
åœ¨å­¦æœ¯åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGemini è¿™æ¬¡ä¸»è¦å¯¹æ ‡ GPT-4ï¼Œåœ¨æ•°å­¦ã€æ¨ç†ã€ä»£ç åŠç»¼åˆèƒ½åŠ›ç­‰æ–¹é¢éƒ½ï¼ˆä»¥å¾®å¼±çš„ä¼˜åŠ¿ï¼‰è¶…è¿‡äº† GPT-4ï¼Œæ¯”å¦‚åœ¨ [MMLU](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) ä»»åŠ¡ä¸Šã€‚Gemini Ultra(CoT@32) å¾—åˆ† 90ï¼Œè€Œä¹‹å‰çš„ SoTA ç”± GPT-4 (5-shots) ä¿æŒï¼Œå¾—åˆ†ä¸º 86.4ï¼Œè¿™ä¹Ÿæ˜¯é¦–æ¬¡æœ‰æ¨¡å‹åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šå¾—åˆ†è¶…è¿‡äººç±»çš„ 89.8 åˆ†ï¼Œè¾¾åˆ°äº† <span style="color:blue"> 90.04 </span> åˆ†!

<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/gemini_ultra_mmlu_20231208_1116.png" width=889pt>
    <figcaption>Multi-Task Language Understanding (MMLU) on Gemini Ultra</figcaption>
</figure>


Gemini å…±æœ‰ä¸‰ä¸ªç‰ˆæœ¬çš„æ¨¡å‹ï¼Œåˆ†åˆ«æ˜¯ï¼š
- Gemini Ultra â€” ç›®å‰å®‡å®™ä¸­æœ€å¼ºå¤§çš„æ¨¡å‹ï¼Œä¸“ä¸ºé«˜åº¦å¤æ‚çš„ä»»åŠ¡è€Œè®¾è®¡ã€‚
- Gemini Pro â€” åœ¨æ€§èƒ½ã€å¤§å°å’Œé€Ÿåº¦ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ï¼Œé€‚ç”¨äºå¤§å¤šæ•°ä»»åŠ¡ã€‚
- Gemini Nano â€” é’ˆå¯¹ç»ˆç«¯è®¾å¤‡è®¾è®¡çš„æ¨¡å‹ï¼Œæ¯”å¦‚åœ¨ Pixel 8 ä¸Šè¿è¡Œã€‚

æ ¹æ® Gemini çš„ [technical report](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)ï¼ŒGemini Pro åŸºæœ¬å¯ä»¥åª²ç¾ GPT-3.5ï¼Œåœ¨ 8 é¡¹è¡Œä¸šæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œæœ‰ 6 é¡¹è¡¨ç°ä¼˜äº GPT-3.5ï¼Œå…¶ä¸­åŒ…æ‹¬ MMLU åŠ GSM8Kã€‚ Google çš„å®˜æ–¹åšå®¢ [Bard gets its biggest upgrade yet with Gemini](https://blog.google/products/bard/google-bard-try-gemini-ai/) æŒ‡å‡º Gemini Pro å·²ç»æ•´åˆåˆ° Bard ä¸Šï¼Œå°½ç®¡å½“å‰ä½¿ç”¨çš„æ˜¯ Gemini Pro è‹±æ–‡æ¨¡å‹çš„å¾®è°ƒç‰ˆæœ¬ï¼Œä½† Gemini Pro çš„å¼•å…¥å·²ç»æ˜¾è‘—å¢å¼ºäº† Bard çš„æ¨ç†ã€è§„åˆ’ä¸ç†è§£èƒ½åŠ›ã€‚é¢„è®¡ Gemini Ultra å°†åœ¨ 2024 å¹´åˆæ•´åˆåˆ° Bard Advanced ä¸Šï¼Œæ„Ÿè§‰åº”è¯¥æ˜¯ Google ç‰ˆ ChatGPT premium çš„é›å½¢ã€‚

Gemini Nano ç»†åˆ†è¿˜æœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼Œä¸€ä¸ª Nano 1, å‚æ•° 1.8Bï¼Œå¦ä¸€ä¸ª Nano 2ï¼Œå‚æ•° 3.25Bã€‚Google åŒæ—¶è¿˜æ”¾å‡ºäº†åŸºäº Gemini çš„ [AlphaCode 2](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf)ï¼Œæ®æŠ¥å‘Šç§°ï¼Œè¾ƒä¸Šä¸€ä»£ AlphaCode å¯ä»¥å¤šè§£å†³ 170% çš„é—®é¢˜ã€‚


# Gemini Pro ä½¿ç”¨ä½“éªŒ 
## è§†é¢‘è§£æèƒ½åŠ›å‡ºä¼—ï¼Œä½†æŒ‡ä»¤å¯¹é½èƒ½åŠ›è¿˜æœ‰å¾…æå‡
ç¬”è€…çº¿ä¸‹æ„Ÿå—äº†ä¸€ä¸‹é›†æˆäº† Gemini Pro çš„æ–°ç‰ˆ Bardï¼Œå®é™…ä½¿ç”¨çš„ä½“éªŒæ˜¯ï¼ŒBard çš„ç†è§£èƒ½åŠ›è¿˜æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ï¼Œæ‹’ç»å›ç­”é—®é¢˜çš„æƒ…å†µä»ç„¶æ™®éï¼Œå°½ç®¡è¿™ä¸ªé—®é¢˜æ˜¯å®ƒå¯ä»¥è½»æ¾è§£å†³çš„ã€‚æ¯”å¦‚ä¸‹é¢æ˜¯ Bard å®˜æ–¹æ¨èçš„ä¸€æ¡åº”ç”¨ç¤ºä¾‹:
```
Give me insights about this video: https://www.youtube.com/watch?v=lr87yrvK86w 
Organize the information in a set of easy to scan bullet points.
```

æ­£å¸¸æƒ…å†µä¸‹ï¼ŒBard ä¼šå¯¹è§†é¢‘è¿›è¡Œåˆ†æï¼Œç„¶åç”Ÿæˆè¿™æ¡è§†é¢‘ç›¸å…³çš„ä¸€äº›è¦ç‚¹ã€‚ä½†å¦‚æœæˆ‘ä»¬æŠŠè§†é¢‘é“¾æ¥æ¢æˆå…¶ä»– YouTube é“¾æ¥ï¼Œæ¯”å¦‚ [Dr. Andrej Karpathy çš„ State of GPT YouTube è§†é¢‘](https://youtu.be/bZQun8Y4L2A?si=8D0NjPDZaLkbXW-f)ï¼Œå†åœ¨ prompt åé¢åŠ ä¸Š â€œreply in Chineseâ€ï¼š

```
Give me insights about this video: https://youtube.com/watch?v=bZQun8Y4L2A
Organize the information in a set of easy to scan bullet points and reply in Chinese.
```

Bard å°±ä¸çŸ¥æ‰€æªäº†ï¼Œåªå›ç­”ï¼š
```
I'm not able to help with that, as I'm only a language model.
``` 

äº‹å®ä¸Š Bard æ˜¯æ”¯æŒå›å¤ä¸­æ–‡çš„ï¼Œä¸Šé¢çš„ prompt å¤šæ¬¡å°è¯•åï¼Œæœ‰æœºç‡å¯ä»¥å¾—åˆ°æ­£å¸¸å›å¤ï¼š

```
è§†é¢‘â€œState of GPT | BRK216HFSâ€çš„è§è§£ ...
```

## è®¡ç®—ã€æ¨ç†èƒ½åŠ›å¼ºï¼Œçº é”™èƒ½åŠ›å¼±
ä¹‹å‰æˆ‘ä»¬åœ¨ [Step-Back Prompting]({{site.baseurl}}/2023/12/02/Step-back-prompting/) ä¸€æ–‡ä¸­æ¢è®¨äº†é€šè¿‡ step-back prompting æé«˜ LLM çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨ ChatGPT ä¸Šè¿›è¡Œäº†ä¸€äº›æµ‹è¯•ï¼Œå…¶ä¸­ä¸€ä¸ªæµ‹è¯•æ ·ä¾‹æ˜¯ä¸€é“é«˜ä¸­ç‰©ç†é¢˜ï¼š

| å¦‚æœæ¸©åº¦å¢åŠ  2 å€ï¼Œä½“ç§¯å¢åŠ  8 å€ï¼Œç†æƒ³æ°”ä½“çš„å‹å¼º $$P$$ ä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ï¼Ÿ

ChatGPT åœ¨ç›´æ¥å›ç­”é—®é¢˜æ—¶ï¼Œæœ‰æ—¶å€™ä¼šå‡ºç°ä¸€äº›æ¦‚ç‡æ€§çš„é”™è¯¯ï¼Œå¯èƒ½æ˜¯æ•°å­¦è®¡ç®—å‡ºé”™ï¼Œæˆ–è€…å…¶ä»–å¥‡æ€ªçš„é”™è¯¯ã€‚ä½†ç»è¿‡æç¤ºä¹‹åï¼Œå®ƒé€šå¸¸èƒ½å¤Ÿè¿…é€Ÿçº æ­£å‰é¢çš„å¤±è¯¯ã€‚ä½†æ˜¯ Bard å°±æ¯”è¾ƒè‡ªä¿¡ï¼ˆé¡½å›ºï¼‰ï¼Œæ— è®ºå¦‚ä½•æç¤ºï¼Œéƒ½ä¸ä¼šæ”¹å˜é”™è¯¯çš„ç­”æ¡ˆã€‚åœ¨è¿™ä¸ªé—®é¢˜ä¸­ï¼Œæ­£ç¡®ç­”æ¡ˆåº”è¯¥æ˜¯â€œå‹å¼ºé™ä¸ºåŸæ¥çš„1/4â€ï¼Œä½† Bard ä¸€å£å’¬å®šæ˜¯â€œå‹å¼ºé™ä¸ºåŸæ¥çš„ 1/8 å€â€ï¼š

<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/too_confident_20231208_1109.png" width=769pt>
</figure>

# å®˜æ–¹åŸºå‡†æµ‹è¯•
Gemini æ”¯æŒæ–‡å­—ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘åŒ…æ‹¬è·¨æ¨¡å‹ï¼Œä¸”åœ¨æ¯ä¸ªæ¨¡æ€ä¸Šçš„è¡¨ç°éƒ½å¾ˆå¼ºã€‚Gemini åœ¨ 12 æ–‡æœ¬å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„ 10 é¡¹ï¼Œ9 é¡¹å›¾åƒç†è§£åŸºå‡†æµ‹è¯•ï¼Œ6 é¡¹è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠ 5 é¡¹è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘åŸºå‡†æµ‹è¯•ä¸­éƒ½åˆ·æ–°äº† SoTAã€‚

åœ¨ [Technical Report](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) ä¸­çš„ç¬¬ä¸€ä¸ªä¾‹å­é‡Œï¼ŒGoogle å±•ç¤ºäº† Gemini æˆåŠŸçš„ä»å›¾ç‰‡ä¸­è¯†åˆ«å‡ºå­¦ç”Ÿæ½¦è‰çš„å­—ä½“ï¼ŒéªŒè¯å­¦ç”Ÿçš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶æ ¹æ®è¦æ±‚ç»™å‡ºäº†ç¬¦åˆæ ¼å¼çš„è§£ç­”ã€‚è¿™ä¸ªä»»åŠ¡å¯ä»¥ç®—æ˜¯ç°å®ä¸–ç•Œä¸­ä¸€ä¸ªæ¯”è¾ƒå¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦æ¨¡å‹å…·å¤‡å¼ºå¤§çš„å›¾ç‰‡è¯†åˆ«ã€æ–‡æœ¬ç†è§£åŠé€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œè€Œ Gemini åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°éå¸¸å‡ºè‰²ã€‚


<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/gemini_example_1_20231207_1505.png" width=789pt>
    <figcaption>Example 1: Gemini answers a question about a studentâ€™s handwriting</figcaption>
</figure>

ä¸€äº›æ¨¡å‹çš„å¤šæ¨¡æ€æ˜¯é€šè¿‡è®­ç»ƒç»„åˆå¤šä¸ªä¸åŒçš„å­æ¨¡å‹å®ç°ï¼Œåœ¨åº”ç”¨ä¸­æ ¹æ®ä¸åŒçš„è¾“å…¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹ï¼Œè€Œ Gemini åœ¨è®­ç»ƒæ—¶å°±æ˜¯é€šè¿‡è·¨æ¨¡æ€è®­ç»ƒçš„ï¼Œè®­ç»ƒæ—¶ä¼šäº¤å‰è¾“å…¥æ–‡æœ¬ã€éŸ³é¢‘åŠè§†è§‰è¾“å…¥ï¼Œåè€…åŒ…æ‹¬å›¾ç‰‡ã€å›¾è¡¨ã€æˆªå›¾ã€PDF åŠè§†é¢‘ï¼Œè¾“å‡ºå¯ä»¥æ˜¯æ–‡æœ¬æˆ–è€…å›¾ç‰‡ã€‚

é‚£ä¹ˆä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œè¿™ç§è·¨æ¨¡æ€çš„è®­ç»ƒæ–¹å¼å’Œä¸“é—¨è®­ç»ƒå•ä¸€é¢†åŸŸæ¨¡å‹çš„æ–¹å¼ç›¸æ¯”ï¼Œå“ªä¸€ä¸ªæ›´å¥½å‘¢ã€‚Google ç§°å‰è€…æ•ˆæœæ›´å¥½ï¼Œå› ä¸º Gemini åœ¨ä¸€ç³»åˆ—æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­éƒ½å–å¾—äº† SoTAã€‚

ğŸ¤” è¾¾åˆ°äº† SoTA æ˜¯ä¸é”™ï¼Œç›´è§‚æ„Ÿè§‰ä¹Ÿæ²¡ä»€ä¹ˆé—®é¢˜ï¼Œä½†æ²¡æœ‰ä½¿ç”¨ç›¸åŒæ•°æ®è®­ç»ƒä¸€ä¸ªå•æ¨¡æ€æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œè¿™ä¸ªç†ç”±æœ‰äº›ç‰µå¼ºã€‚


<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/crossmodal_20231208_1122.png" width=778pt>
</figure>

## è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡
<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/text_benchmark_20231208_1124.png" width=789pt>
    <figcaption>Text benchmarks</figcaption>
</figure>

å¦‚ä¸Šè¡¨æ‰€ç¤ºï¼ŒGemini Ultra åœ¨å‡ é¡¹æ¯”è¾ƒé‡è¦çš„åŸºå‡†æµ‹è¯•è¡¨ç°ä¸­éƒ½æ¯”è¾ƒæŠ¢çœ¼ï¼Œæœ€ä¸ºçªå‡ºçš„æ— ç–‘æ˜¯åœ¨ MMLU ä»»åŠ¡ä¸Šé¦–æ¬¡çªç ´ 90 åˆ†å¤§å…³ï¼Œæ¯”ä¹‹å‰çš„ GPT-4 è¿˜è¦é«˜å‡º 3 ä¸ªç‚¹ã€‚ è¿™ä¸ªçªç ´ä¸»è¦åˆ©ç›Šäº Gemini Ultra ä½¿ç”¨çš„ä¸€ä¸ª *uncertainty-routed chain-of-thought* ç­–ç•¥ï¼Œè¿™ä¸ªç­–ç•¥åœ¨ techinical report å‡ å¥å¸¦è¿‡ï¼Œæ²¡æœ‰è¯¦ç»†è§£é‡Šï¼Œä¹Ÿæ²¡æœ‰ç¤ºä¾‹ã€‚Uncertainty-rounted CoT çš„åŸæ–‡å¦‚ä¸‹ï¼š

```text
...
We proposed a new approach where model produces k chain-of-thought samples, selects the majority vote if the model is confident above a threshold, and otherwise defers to the greedy sample choice. The thresholds are optimized for each model based on their validation split performance. 
...
```

ç¬”è€…æš‚æ—¶è¿˜æ²¡æœ‰å®Œå…¨ç†è§£å…·ä½“æ˜¯å¦‚ä½•æ“ä½œçš„ï¼Œä½†çœ‹èµ·æ¥æœ‰ç§é¢„æµ‹å¤šä¸ªç»“æœç„¶åæŠ•ç¥¨ï¼ˆconsensus votingï¼‰çš„æ„æ€ã€‚NVIDIA çš„ä¸€ä¸ª AI å·¥ç¨‹å¸ˆ [twitter@Sergio Perez](https://x.com/sergiopprz/status/1732502923022684501?s=20) çš„è§è§£ç±»ä¼¼ï¼š

```text
With the "uncertainty-routed" approach, the model generates several answers, each of them with their own CoT. If there's enough consensus among the answers, the model chooses that answer, and if not it reverts to simple maximum-likelihood sampling (i.e. no CoT) at all.
```

è¿™ä¸ªç­–ç•¥çš„æ•ˆæœå®åœ¨æ˜¯è¿‡äºæ˜¾è‘—ï¼Œå› ä¸º Gemini Ultra + 5-shot æ•ˆæœäº‹å®ä¸Šæ¯” GPT-4 + 5-shot ä½ 2.7 ä¸ªç‚¹ï¼Œä½† Gemini Ultra + 32-shot + CoT å†åŠ ä¸Šä¸Šé¢çš„ consensus voting çš„æ–¹æ³•å´åè¶…äº† GPT-4 3 ä¸ªç‚¹ï¼ŒGoogle è¿™æ¬¡ä¸ºäº†åœ¨ MMLU ä¸Šè¶…è¶Š GPT-4 å¯è°“æ˜¯â€œä¸‹è¶³äº†åŠŸå¤«â€ã€‚å› æ­¤æœ‰äººï¼ˆe.g., [twitter@yi_ding](https://twitter.com/yi_ding/status/1732443815804653744)ï¼‰è¡¨ç¤ºè´¹è§£çš„åŒæ—¶ï¼Œä¹Ÿæœ‰äººå¯¹ Gemini çš„è®­ç»ƒåŠæµ‹è¯•è¿‡ç¨‹è¡¨ç¤ºäº†è´¨ç–‘ã€‚ 

<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/satire_gemini_20231207_2130.png" width=389pt>
    <figcaption>The magic behind Gemini Ultra</figcaption>
</figure>   

å¦å¤–ä¸€ä¸ªæ¯”è¾ƒæ˜¾çœ¼çš„ç»“æœæ˜¯åœ¨ GSM8K ä¸Šè¾¾åˆ°äº† 94.4% çš„å‡†ç¡®ç‡ï¼Œç­–ç•¥æ˜¯ CoT + [self-consistency](https://arxiv.org/pdf/2203.11171.pdf)ï¼Œå°±æ˜¯é¢„æµ‹å¤šä¸ªç»“æœï¼Œç„¶åæŠ•ç¥¨ã€‚æ•ˆæœç¡®å®æ˜¯å¥½äº†ï¼Œä½†è¿™ç§æ–¹å¼è®¡ç®—é‡æœ‰ç‚¹å¤§ã€‚ 

## ç¼–ç¨‹èƒ½åŠ›
Google åŒæ—¶è¿˜æ¨å‡ºåŸºäº Gemini Pro å¾®è°ƒçš„ [AlphaCode 2](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf)ï¼Œ AlphaCode 2 çš„ä¸»è¦ç»„ä»¶åŒ…æ‹¬ï¼š 
- ç­–ç•¥æ¨¡å‹ï¼ˆpolicy & fine-tuningï¼‰ï¼Œä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆä»£ç ç¤ºä¾‹ï¼› 
- é‡‡æ ·æœºåˆ¶ï¼ˆsamplingï¼‰ï¼Œé¼“åŠ±ç”Ÿæˆå¹¿æ³›å¤šæ ·çš„ä»£ç ç¤ºä¾‹ä»¥æœç´¢å¯èƒ½ç¨‹åºçš„ç©ºé—´ï¼› 
- è¿‡æ»¤æœºåˆ¶ï¼ˆfilteringï¼‰ï¼Œç”¨äºåˆ é™¤ä¸é—®é¢˜æè¿°ä¸ç¬¦çš„ä»£ç ç¤ºä¾‹ï¼› 
- èšç±»ç®—æ³•ï¼ˆclusteringï¼‰ï¼Œå°†è¯­ä¹‰ç›¸ä¼¼çš„ä»£ç ç¤ºä¾‹åˆ†ç»„ï¼Œä»è€Œé¿å…å†—ä½™ï¼› 
- è¯„åˆ†æ¨¡å‹ï¼ˆscoringï¼‰ï¼Œç”¨äºä»æ¯ä¸ªå‰10ä¸ªä»£ç ç¤ºä¾‹é›†ç¾¤ä¸­å‘ˆç°æœ€ä½³å€™é€‰é¡¹ã€‚

æŠ¥å‘Šç§°è¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨ç«æŠ€ç¼–ç¨‹ä¸­è¾¾åˆ° expert æ°´å¹³çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚åœ¨ Codeforces ä¸Šæµ‹è¯•æ—¶ï¼Œç»™ 10 æ¬¡å°è¯•æœºä¼šï¼ŒAlphaCode 2 æœ‰ 43% çš„æ¦‚ç‡ ACï¼Œè€Œä¹‹å‰çš„ AlphaCode åªæœ‰ 25% çš„æ¦‚ç‡ ACã€‚AlphaCode 2 coding èƒ½åŠ›è¿™ä¹ˆå¼ºï¼Œä½†ä¸ºä»€ä¹ˆæ²¡æœ‰æ”¾å‡ºæ¥è®©å¤§å®¶ç”¨ç”¨ï¼ŸGoogle åœ¨ report é‡Œè¯´äº†ï¼šâ€œOur system requires a lot of trial and error, and remains too costly to operate at scale. Further, it relies heavily on being able to filter out obviously bad code samples.â€ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯è¿˜ä¸å¤ªç¨³å®šï¼Œä¸”æˆæœ¬å¤ªé«˜ï¼Œä»ç„¶éœ€è¦ç»§ç»­ä¼˜åŒ–ã€‚

å®é™…ä¸Šï¼Œä»”ç»†è§‚å¯Ÿä¸€ä¸‹ä¸Šé¢çš„ç³»ç»Ÿç»“æ„ï¼Œå¯ä»¥çœ‹åˆ°åœ¨ policy ä¸ scoring é˜¶æ®µéƒ½éœ€è¦ä¸€ä¸ªæ¨¡å‹ï¼Œè€Œ AlphaCode 2 å®é™…ä¸Šåœ¨è¿™ä¸¤ä¸ªé˜¶æ®µåˆ†åˆ«å¾®è°ƒäº†ä¸€ä¸ª Gemini Pro æ¨¡å‹ï¼Œåœ¨ clustering é˜¶æ®µï¼Œä¹Ÿéœ€è¦å†è®­ç»ƒä¸€ä¸ªæ¨¡å‹è¿›è¡Œèšç±»ã€‚ä¸¤ä¸ª Gemini Pro + ä¸€ä¸ªèšç±»æ¨¡å‹ï¼Œè¿™ä¸ªæˆæœ¬ç¡®å®ä¸ä½ã€‚

# ç¬¬ä¸‰æ–¹åŸºå‡†æµ‹è¯• 
ä¸Šä¸€ç« çš„ç»“æœéƒ½æ˜¯ Google äº  2023-12-06 åœ¨ [technical report](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) ä¸­å‘å¸ƒçš„ï¼Œè€Œæœ€è¿‘ä¸¤æ—¥ï¼ˆ2023-12-19ï¼‰Aran Komatsuzaki åœ¨ [Twitter](https://twitter.com/arankomatsuzaki/status/1736998198697238672?s=51) ä¸Šåˆ†äº«äº†ä¸€é¡¹ç”± [CMU](https://cmu.edu), [Zeno](https://zenoml.com), and [BerriAI LiteLLM](https://github.com/BerriAI/litellm) è”åˆå®Œæˆçš„å·¥ä½œ [ã€ŠAn In-depth Look at Gemini's Language Abilitiesã€‹](https://arxiv.org/abs/2312.11444)ã€‚åœ¨è¿™ä¸ªå·¥ä½œé‡Œï¼Œä»–ä»¬é‡æ–°å¯¹æ¯”äº† Gemini ä¸ GPT çš„æ•ˆæœï¼Œå…³äº Gemini çš„ç»“è®ºæ˜¯ï¼š

* **æ•´ä½“èƒ½åŠ›å¼±äº GPT 3.5**ï¼šGoogle Pro åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šæ•ˆæœéƒ½æ¥è¿‘ä½†å¼±äº GPT 3.5 Turboã€‚
* **ä¸ç¨³å®š**ï¼šåœ¨å¤„ç†å…·æœ‰å¤šä¸ªæ•°å­—çš„æ•°å­¦æ¨ç†ä»»åŠ¡æ—¶ï¼ŒGemini æœ‰æ—¶å€™ä¸èƒ½å®Œæˆä»»åŠ¡ã€‚åœ¨å¤šç­”æ¡ˆé—®é¢˜æ’åºã€æ¿€è¿›å†…å®¹è¿‡æ»¤ç­‰ä»»åŠ¡ä¸Šä¹Ÿæ¯”è¾ƒä¸ç¨³å®šã€‚
* **åœ¨éƒ¨åˆ†é¢†åŸŸèƒ½åŠ›å‡ºä¼—**ï¼šåœ¨ç”Ÿæˆéè‹±è¯­è¯­è¨€ã€å¤„ç†æ›´é•¿ã€æ›´å¤æ‚çš„æ¨ç†é“¾ä»¥åŠè§£å†³å•è¯æ’åº/é‡æ–°æ’åˆ—é—®é¢˜æ–¹é¢ï¼ŒGemini å±•ç°å‡ºäº†ç›¸å½“é«˜çš„æ°´å¹³ã€‚


ç¬¬ä¸€æ¡ç»“è®ºå¯¹åº”æ•°æ®å¦‚ä¸‹è¡¨ï¼ŒGPT 3.5 Turbo åœ¨ 12 ç»„å®éªŒä¸­å®Œèƒœ Gemini Proï¼Œå®Œå…¨æ˜¯æŠŠ Gemini æ‘åœ¨åœ°ä¸Šæ‘©æ“¦çš„èŠ‚å¥ï¼Œç‰¹åˆ«æ˜¯ MMLU(CoT) çš„æ•ˆæœï¼Œä»¥ 10 ä¸ªç‚¹çš„ä¼˜åŠ¿é¥é¥é¢†å…ˆäº Geminiã€‚

<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/far_ahead_20231222_0952.png" width="334pt">
</figure> 

è¿™ä¸ªç»“è®ºç›¸æ‚–äº Gemini Team çš„ç»“è®ºï¼Œä¼°è®¡ Google ä¸ä¼šå–œæ¬¢ã€‚ä¸è¿‡è¿™æ¬¡ç¬¬ä¸‰æ–¹å®éªŒè®¾è®¡ä¹Ÿæ¯”è¾ƒå…¬æ­£ï¼Œæ²¡æœ‰ä»€ä¹ˆæ§½ç‚¹ï¼Œä»é—®é¢˜åˆ° prompts å†åˆ°è¯„ä¼°æ–¹å¼éƒ½ä¸€æ ·ï¼Œä¸åƒ Google æ‹¿ Gemini + uncertainty-routed CoT å¯¹æ¯” GPT-4 + CoTï¼Œç»“æœéš¾ä»¥æœä¼—ã€‚


| Task| Dataset| Gemini Pro | GPT 3.5 Turbo | GPT 4 Turbo | Mixtral |
|---|---|---|----|---|---|
| **Knowledge-based QA** | **MMLU (5-shot)**   | 64.12  | 67.75 | **80.48**   | - |
|      | **MMLU (CoT)**      | 60.63      | 70.07         | **78.95**   | -       |
| **Reasoning**  | **BIG-Bench-Hard**  | 65.58      | 71.02         | **83.90**   | 41.76   |
| **Mathematics**               | **GSM8K**           | 69.67      | 74.60         | **92.95**   | 58.45   |
|                               | **SVAMP**           | 79.90      | 82.30         | **92.50**   | 73.20   |
|                               | **ASDIV**           | 81.53      | 86.69         | **91.66**   | 74.95   |
|                               | **MAWPS**           | 95.33      | **99.17**     | 98.50       | 89.83   |
| **Code Generation**           | **HumanEval**       | 52.44      | 65.85         | **73.17**   | -       |
|                               | **ODEX**            | 38.27      | 42.60         | **46.01**   | -       |
| **Machine Translation**       | **FLORES (0-shot)** | 29.59      | 37.50         | **46.57**   | -       |
|                               | **FLORES (5-shot)** | 29.00      | 38.08         | **48.60**   | -       |
| **Web Agents**                | **WebArena**        | 7.09       | 8.75          | **15.16**   | 1.37    |

ç ”ç©¶äººå‘˜åˆ†æäº† Gemini underperformance çš„åŸå› ï¼Œä¸€ä¸ªåŸå› æ˜¯æ— æ³•å›ç­”é—®é¢˜ï¼ˆè¿™ä¸€ç‚¹ç¬”è€…åœ¨ Bard çš„å®éªŒä¸­ä¹Ÿé‡åˆ°äº†ï¼‰ï¼Œç‰¹åˆ«æ˜¯è·Ÿé“å¾·åœºæ™¯ï¼ˆmoral scenariosï¼‰åŠäººç±»æ€§è¡Œä¸ºï¼ˆhuman sexualityï¼‰ç›¸å…³çš„é—®é¢˜ï¼ŒGemini ç»å¸¸æ‹’ç»å›ç­”ï¼Œä¼°è®¡ Google ä¹‹å‰åƒäº†å®¡æŸ¥ä¸ä¸¥çš„äºï¼ˆæ¯”å¦‚ä¹‹å‰çš„[â€œå¤§çŒ©çŒ©äº‹ä»¶â€](https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai)ï¼‰ï¼Œæ‰€ä»¥è¿™æ¬¡åœ¨å®¡æŸ¥ä¸Šåšäº†å¾ˆå¤šé™åˆ¶ã€‚å¦ä¸€ä¸ªåŸå› æ˜¯ Gemini æ•°å­¦èƒ½åŠ›å¾ˆå·®ï¼Œè¿™ä¸€ç‚¹ï¼Œç¬”è€…åœ¨æ–‡ç« [ã€ŠTree of Thoughtsã€‹]({{site.baseurl}}/2023/12/10/Tree-of-Thoughts/)ä¸­ä¹Ÿæœ‰æåˆ°ï¼Œåš 24 ç‚¹æ¸¸æˆæ—¶ï¼Œ Gemini åœ¨æ•°å­¦æ¨ç†ä¸Šçš„è¡¨ç°ç¡®å®ä¸å¦‚ ChatGPTï¼ˆåº•å±‚æ¨¡å‹ GPT 3.5 Turboï¼‰ã€‚


# The LLM war is not over yet
ä» 2022/11/30 OpenAI å‘å¸ƒ ChatGPTï¼Œå¦‚ä»Šå·²æœ‰ä¸€å¹´æ—¶é—´ï¼ŒGPT-4 å‘å¸ƒä¹Ÿå·®ä¸å¤šå·²æœ‰ 8 ä¸ªæœˆï¼Œåœ¨å½“å‰ä¼—å¤šå¤§æ¨¡å‹æ··æˆ˜çš„æƒ…å†µä¸‹ï¼ŒGemini è¡¨ç°å‡ºäº†ä¸€å®šç«äº‰åŠ›ï¼Œä½†å¯¹æ¯”ä¸€å¹´å‰çš„ GPT 3.5 ~~ä¹Ÿå°±ä»¥å¾®å¼±çš„ä¼˜åŠ¿é¢†~~å¹¶æ²¡æœ‰ä½“ç°å‡ºä¼˜åŠ¿ï¼Œè€Œä¸” Bard çš„è¡¨ç°å¯¹æ¯” ChatGPT è¿˜å·®å¾ˆè¿œã€‚ç­‰åˆ° 24 å¹´åˆ Gemini Ultla æœ€ç»ˆäº®ç›¸çš„æ—¶å€™ï¼ŒOpenAI æˆ–è€… Anthropic Inc. å¯èƒ½ä¼šå†æ¬¡æ€èµ·æ–°ä¸€è½®çš„æ¨¡å‹å‘å¸ƒçƒ­æ½®ï¼Œåˆ°æ—¶å€™åˆæ˜¯ä¸€åœºè…¥é£è¡€é›¨çš„æˆ˜äº‰ã€‚

# å‚è€ƒ 
- AlphaCode 2 technical report: [https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf)
- Gemini 1.0 technical report: [https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)
- Bard gets its biggest upgrade yet with Gemini: [https://blog.google/products/bard/google-bard-try-gemini-ai/](https://blog.google/products/bard/google-bard-try-gemini-ai/)
- The Best AI Model in the World: Google DeepMindâ€™s Gemini Has Surpassed GPT-4: [https://albertoromgar.medium.com/the-best-ai-model-in-the-world-google-deepminds-gemini-has-surpassed-gpt-4-1ee07f84d2ff](https://albertoromgar.medium.com/the-best-ai-model-in-the-world-google-deepminds-gemini-has-surpassed-gpt-4-1ee07f84d2ff)
- Introducing Gemini: our largest and most capable AI model: [https://blog.google/technology/ai/google-gemini-ai/#capabilities](https://blog.google/technology/ai/google-gemini-ai/#capabilities)

