---
layout: post
title: T5
date: 2023-02-28
tags: 
categories: nlp
author: GaoangLiu
---
* content
{:toc}


Text-to-Text Transfer Transformer，由 Google Research （论文:[https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683)，开源代码: [https://github.com/google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer)） 于 2020 提出的生成式预训练模型，理念是“万物皆可 seq2seq”，目标是统一框架，将文本分类、生成、翻译等诸多任务都转化成 text-to-text 任务。这样即可以用**同样的模型，同样的损失函数，同样的训练过程，同样的解码过程**来解决所有的 NLP 任务（GPT-2 是有类似思想）。 




要统一框架，一个要解决的问题是：如何将不同的任务转化为 text-to-text 任务。T5 的做法是：**为输入序列增加前缀** `<prefix>`。如下面的例子所示，前缀表达的意思包括：

- translate English to German: + [sequence]：翻译任务。
- cola sentence: + [sequence]： CoLA 语料库，微调 BERT 模型。
- stsb sentence 1:+[sequence]：语义文本相似基准。自然语言推理和蕴涵是类似的问题。
- summarize + [sequence]：文本摘要问题。
- multilabel classification: + [sequence]：多标签分类问题。

这样，即得到了一个统一的 NLP 任务格式：

```bash
prefix + sentence A ---> sentence B
```

![t5](https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s1600/image3.gif)


# 结构
首先，仍然是 Encoder-Decoder 结构。 

其次，使用 BERT-style 进行训练，采用的 replace span （小段替换）法，即将一小段连续的 token 替换成一个特殊符号，可以提高计算效果。 

Mask, replace span, dropout 对比

| method | text |
| :---- |:---|
| original |  exploring the limits of transfer learning with a unified text-to-text transformer|
| mask | exploring the limits of \<M\> \<M\> with a unified \<M\> transfomer|
| replace span | exploring the limits of \<X\> with a \<X\> | 
| dropout | exploring the limits of with a unified | 

replace span 的比例为 15%，小段长度在 2, 3, 5, 10 这 4 个值中选择了 3。

总结而言：
- Transformer Encoder-Decoder 模型
- BERT-style 式的破坏方法
- Replace Span 的破坏策略
- 15 %的破坏比
- 3 的破坏时小段长度

一些细节：
- Transformer 使用了正余弦位置编码，BERT 使用的是学习到的位置编码，T5 使用了**相对位置嵌入**。


## 训练过程 
T5 的预训练包含无监督和有监督两部分。

无监督部分使用的是 Google 构建的近 800G 的语料（Colossal Clean Crawled Corpus, C4），训练目标则跟 BERT 类似，只不过改成了 Seq2Seq 版本，可以将它看成一个高级版的完形填空问题：

**input**: exploring the [M0] of [M1] with a unified [M2] transformer

**output**: [M0] limits [M1] transfer learning [M2] text-to-text

监督训练则将多种 NLP 任务转在 seq2seq 任务进行训练，例如对于阅读理解任务（model reading comprehension, mrc），输入为： “阅读理解：小明的爸爸有三个儿了，大儿子叫大聪，二儿子叫二聪。问题：三儿子叫什么？ 答案：”。


# 扩展
## mT5
Multilingual T5，T5 的多语言版本，支持 101 种语言，使用的语料 [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual_nights_stay)。论文 [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934)，Github 链接 [https://github.com/google-research/multilingual-t5](https://github.com/google-research/multilingual-t5)。



# 参考
- [Github, Google Research, T5 model](https://github.com/google-research/text-to-text-transfer-transformer)
- [T5 Paper, Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
- [Towardsdatascience, The Guide to Multi-Tasking with the T5 Transformer](https://towardsdatascience.com/the-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b)
- [Googleblog, Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
- [腾讯开发者社区, 【长文详解】T5: Text-to-Text Transfer Transformer 阅读笔记](https://cloud.tencent.com/developer/article/1537682)
