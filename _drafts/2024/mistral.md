---
layout:     post
title:      The mysterious Mistral (WIP)
date:       2024-01-11
tags: [mistral, moe]
categories: 
- nlp
---





Mistral.ai åœ¨ arXiv ä¸Šæ”¾å‡ºæ¥è®ºæ–‡ [Mixtral of Experts](https://arxiv.org/pdf/2401.04088.pdf)ã€‚

Q: å¤§ä½“æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ

Q: router network æ˜¯æ€ä¹ˆå·¥ä½œçš„ï¼Ÿæ€ä¹ˆé€‰æ‹©ï¼Œæ€ä¹ˆç»„åˆï¼Ÿ
a router network selects two experts to process the current state and combine their outputs. 

Q: ä¸ºä»€ä¹ˆè¦ç”¨router networkï¼Ÿ

Q: ä¸ºä»€ä¹ˆè¦ç”¨ä¸¤ä¸ªexpertï¼Ÿ


# Mistral 7B
è®ºæ–‡: https://arxiv.org/pdf/2310.06825.pdf, arXiv 23.10

é¦–å…ˆè¯´ä¸€ä¸‹å½“æ—¶è¿™ç¯‡å·¥ä½œçš„äº®ç‚¹ï¼š
1. åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå½“æ—¶æœ€ä½³å¼€æºæ¨¡å‹ Llama 2 13Bï¼Œåœ¨æ¨ç†ã€æ•°å­¦å’Œä»£ç ç”Ÿæˆæ–¹é¢ä¼˜äº Llama 1 34B
2. åˆ©ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰æ¥å®ç°æ›´å¿«çš„æ¨ç†ï¼Œå¹¶ç»“åˆæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆSWAï¼‰æ¥æœ‰æ•ˆåœ°å¤„ç†ä»»æ„é•¿åº¦çš„åºåˆ—ï¼ŒåŒæ—¶é™ä½æ¨ç†æˆæœ¬
3. æä¾›äº†ä¸€ä¸ªç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹ï¼ŒMistral 7B-Instructï¼Œåœ¨äººç±»å’Œè‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•ä¸Šéƒ½è¶…è¶Šäº† Llama 2 13B èŠå¤©æ¨¡å‹


# Sparse Mixtures of Experts
ç»™å®šè¾“å…¥ $x$ï¼ŒMoE æ¨¡å—çš„è¾“å‡º $y=\sum_{i=0}^{n-1} G(x)_i \cdot E_i(x)$ï¼Œå…¶ä¸­ $n$ æ˜¯ä¸“å®¶ç½‘ç»œï¼ˆä¸‹ç§°ä¸“å®¶ï¼‰çš„ä¸ªæ•°ï¼Œ$G(x)$ æ˜¯ç¬¬$i$ä¸“å®¶çš„æƒé‡ï¼Œ$E_i(x)$ æ˜¯ç¬¬ $i$ ä¸ªä¸“å®¶çš„è¾“å‡ºã€‚$G(x)$ çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªæ¦‚ç‡å€¼ï¼Œä¸”æ»¡è¶³ $\sum_{i=0}^{n-1} G(x)_i = 1$ã€‚

é‚£ SMoE ä¸­çš„ sparse æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿå…¶å®å°±æ˜¯æŒ‡åªæœ‰å°‘æ•°ä¸“å®¶å‚ä¸å†³ç­–ï¼Œå³ $G(x)$ ä¸­çš„å¤§éƒ¨åˆ†å…ƒç´ éƒ½æ˜¯ 0ï¼Œåªæœ‰å°‘éƒ¨åˆ†éé›¶ã€‚è¿™æ ·çš„è¯ï¼Œå°±å¯ä»¥å‡å°‘è®¡ç®—é‡ã€‚æ—¢ç„¶åªæœ‰å°‘æ•°ä¸“å®¶å‚ä¸å†³ç­–ï¼Œé‚£å°±éœ€è¦ä¸€ä¸ªç­–ç•¥å†³å®šå“ªäº›ä¸“å®¶å‚ä¸å†³ç­–ï¼Œæ–¹æ³•æœ‰å¾ˆå¤šç§ï¼Œä¸€ç§ç®€å•é«˜æ•ˆçš„æ–¹æ³•æ˜¯å¯¹çº¿æ€§å±‚çš„å‰Kä¸ªlogitsåº”ç”¨softmaxå‡½æ•°ï¼š

$$G(x)=\text{Softmax}(\text{TopK}(x \cdot W_g))$$

$k$ åšä¸ºä¸€ä¸ªè¶…å‚æ•°ï¼Œå¯ä»¥é€šè¿‡å¹³è¡¡æ•ˆæœä¸è®¡ç®—é‡æ¥è°ƒæ•´ã€‚Mistral 8x7B ä¸­ä½¿ç”¨çš„æ˜¯ $k=2$ï¼Œå³åªæœ‰ä¸¤ä¸ªä¸“å®¶å‚ä¸å†³ç­–ã€‚


### ğŸ“ ä¸€äº›ä¸å†³ç­–ç›¸å…³çš„å·¥ä½œ 
- [Unified scaling laws for routed language models](https://arxiv.org/abs/2202.01169)
- [Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning](https://proceedings.neurips.cc/paper/2021/hash/f5ac21cd0ef1b88e9848571aeb53551a-Abstract.html)
- [CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge](https://arxiv.org/abs/1811.00937)
 
Mistral çš„æ¨¡å‹ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<figure style="text-align: center;">
    <img src="https://image.ddot.cc/202401/mistral-moe-layer_20240111_1038.png" width=789pt>
    <figcaption style="text-align:center"> Mistral MoE layer ç»“æ„å›¾ </figcaption>
</figure>