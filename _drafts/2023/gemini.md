---
layout:     post
title:      Gemini, The best AI model in the world 
date:       2023-12-06
tags:   [gemini, llm, deepmind]
categories: 
- nlp
---

# Google is so back

ä» [ChatGPT](https://chat.openai.com) å‘å¸ƒä¹‹åï¼Œç›¸å½“ä¸€éƒ¨åˆ†ç”¨æˆ·çš„æœç´¢ä¹ æƒ¯å‘ç”Ÿäº†æ”¹å˜ï¼Œä¸å†æ˜¯ç›´æ¥åœ¨ Google æœç´¢ï¼Œè€Œæ˜¯ç›´æ¥å‘ ChatGPT æé—®ï¼ŒGoogle æœç´¢æµé‡æ›¾ä¸€åº¦å¤§å¹…ä¸‹é™ï¼ŒChatGPT å¯¹ Google æœç´¢å¨èƒä¹‹å¤§ï¼Œæ®è¯´è¿åˆ›å§‹äººéƒ½äº²è‡ªä¸‹åœºæ LLMï¼ˆ[Back At Google Again, Cofounder Sergey Brin Just Filed His First Code Request In Years â€”â€” Forbes](https://dlj.one/zprxug)ï¼‰ã€‚å…¶å®åœ¨ AI é¢†åŸŸï¼ŒGoogle ä¸€ç›´å¤„äºé¢†å…ˆåœ°ä½ï¼Œæ¯•ç«Ÿæ——ä¸‹æœ‰ Google Brain å’Œ DeepMind ä¸¤å¤§é¡¶çº§ AI ç ”ç©¶æœºæ„ï¼ŒDeepMind æ——ä¸‹çš„ AlphaGo ä¹Ÿæ˜¯å½“å¹´é£å¤´æ— ä¸¤ã€å®¶å–»æˆ·æ™“çš„æ˜æ˜Ÿ AI äº§å“ã€‚åœ¨ NLP é¢†åŸŸï¼ŒGoogle ä¹Ÿé¢‡æœ‰å»ºæ ‘ï¼Œç‡å…ˆæå‡ºäº† transformer ç»“æ„ï¼Œä¹Ÿå…ˆåå‘å¸ƒäº† BERTã€T5ã€ALBERTã€Switch Transformer ç­‰å¤šä¸ªçŸ¥åæ¨¡å‹ï¼Œä½†åœ¨èŠå¤©æœºå™¨äººä¸Šï¼Œå¯¹æ¯” OpenAI çš„ GPT ç³»åˆ—ï¼ŒGoogle å°±æ¯”è¾ƒæ»åã€‚åœ¨ ChatGPT å‘å¸ƒä¸‰ä¸ªæœˆä¹‹åï¼ŒGoogle
æ‰åŒ†å¿™èµ¶ [Bard](https://bard.google.com) ä¸Šæ¶ï¼Œåšä¸º Google çš„ç¬¬ä¸€ä¸ª AI èŠå¤©æœºå™¨äººï¼Œå¤§å®¶å¯¹ Bard å¯„äºˆåšæœ›ï¼Œä½† Bard æ•ˆæœç€æ€¥å¤ªå·®ï¼Œè¿åŸºæœ¬çš„ç®—æœ¯éƒ½é¢‘ç¹å‡ºé”™ï¼Œè¯­è¨€èƒ½åŠ›æ›´æ˜¯æœ‰é™ï¼Œæ—©æœŸçš„ç‰ˆæœ¬åªæ”¯æŒè‹±æ–‡ï¼Œå¯¹æ¯” ChatGPT ç®€ç›´æ˜¯å¤©å£¤ä¹‹åˆ«ï¼Œæœ‰äººç”šè‡³è°ƒä¾ƒç§°ä¸º[â€œBard is a jokeâ€](https://twitter.com/high_byte/status/1639596716339896322)ã€‚ä¸ºäº†è¿½å¹³ OpenAIï¼ŒGoogle ä¸æƒœå°†ä¸¤ä¸ª AI ç ”ç©¶æœºæ„åˆå¹¶ï¼Œæˆç«‹äº† Google DeepMindã€‚åœ¨ä»Šå¹´ 5 æœˆä»½çš„ Google I/O å¤§ä¼šä¸Šï¼ŒGoogle å®£å¸ƒæ–°çš„ Google DeepMind å®éªŒå®¤å·²å¼€å§‹å¼€å‘ Geminiã€‚

æ—¶éš”åŠå¹´ï¼ŒGoogle DeepMind ç»ˆäºæäº†ä¸€ä¸ªå¤§æ–°é—»ï¼Œå‘å¸ƒäº† [Gemini 1.0](https://deepmind.google/technologies/gemini/) å¤šæ¨¡æ€ AI è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶æ”¯æŒæ–‡å­—ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šç§è¾“å…¥æ•°æ®çš„å¤„ç†ã€‚æ ¹æ® CEO Sundar Pichai åœ¨ [twitter](https://twitter.com/sundarpichai/status/1732433036929589301) ä¸Šå‘å¸ƒçš„ Gemini æ•ˆæœå±•ç¤ºè§†é¢‘ä¸­å¯ä»¥çœ‹åˆ°ï¼ŒGemini åœ¨éŸ³é¢‘ã€å›¾ç‰‡è¯†åˆ«ã€æ–‡å­—ç†è§£ã€æ¨ç†æ–¹é¢éƒ½è¡¨ç°å‡ºæƒŠäººçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒGemini åœ¨æ–‡æœ¬ã€éŸ³é¢‘ã€è§†é¢‘åŠç¼–ç¨‹ç­‰ä¸€ç³»åˆ—å¼€æºæ•°æ®é›†ä»»åŠ¡çš„è¯„æµ‹ä¸­éƒ½åˆ·æ–°äº† SoTAï¼Œæ•ˆæœéå¸¸å¥½ï¼Œå¯ä»¥è¯´æ˜¯ç›®å‰æœ€å¼ºç»¼åˆ AI è¯­è¨€æ¨¡å‹äº†ã€‚

Gemini ä¸€å‡ºï¼ŒTwitterï¼ŒYouTubeï¼Œ å¾®ä¿¡å…¬ä¼—å·ï¼Œæœ‹å‹åœˆæ»¡å±å¹•éƒ½æ˜¯ `Gemini`ã€`SoTA`ã€`å¤šæ¨¡æ€`ï¼Œæœ‰ç‚¹å½“åˆ ChatGPT åˆšå¤§ç«ä¹‹åçš„é˜µä»—ï¼Œç»™äººä¸€ç§ Google ç»ˆäºä¸€é›ªå‰è€»ï¼Œä¸€ç»Ÿ AI çš„æ„Ÿè§‰ã€‚

<figure style="text-align: center;">
    <img src="https://image.ddot.cc/202312/gemini_openai_20231207_2247.png" width=445pt>
    <figcaption>Google is so back</figcaption>
</figure>

åœ¨å­¦æœ¯åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGemini è¿™æ¬¡ä¸»è¦å¯¹æ ‡ GPT-4ï¼Œåœ¨æ•°å­¦ã€æ¨ç†ã€ä»£ç åŠç»¼åˆèƒ½åŠ›ç­‰æ–¹é¢éƒ½ï¼ˆä»¥å¾®å¼±çš„ä¼˜åŠ¿ï¼‰è¶…è¿‡äº† GPT-4ï¼Œæ¯”å¦‚åœ¨ [MMLU](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) ä»»åŠ¡ä¸Šã€‚Gemini Ultra(CoT@32) å¾—åˆ† 90ï¼Œè€Œä¹‹å‰çš„ SoTA ç”± GPT-4 (5-shots) ä¿æŒï¼Œå¾—åˆ†ä¸º 86.4ï¼Œè¿™ä¹Ÿæ˜¯é¦–æ¬¡æœ‰æ¨¡å‹åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šå¾—åˆ†è¶…è¿‡äººç±»çš„ 89.8 åˆ†ï¼Œè¾¾åˆ°äº† <span style="color:blue"> 90.04 </span> åˆ†!

<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/gemini_ultra_mmlu_20231208_1116.png" width=889pt>
    <figcaption>Multi-Task Language Understanding (MMLU) on Gemini Ultra</figcaption>
</figure>

Gemini æœ‰ä¸‰ä¸ªç‰ˆæœ¬å¤§å°çš„æ¨¡å‹ï¼Œåˆ†åˆ«æ˜¯ï¼š
- Gemini Ultra â€” å®‡å®™å½“å‰æœ€å¼ºæ¨¡å‹ï¼Œé€‚ç”¨äºé«˜åº¦å¤æ‚çš„ä»»åŠ¡ã€‚
- Gemini Pro â€” æ€§èƒ½ã€å¤§å°å’Œé€Ÿåº¦çš„æœ€ä½³å¹³è¡¡ï¼Œé€‚ç”¨äºå¤§å¤šæ•°ä»»åŠ¡ã€‚
- Gemini Nano â€” é€‚åˆç»ˆç«¯è®¾å¤‡ä¸Šçš„æ¨¡å‹ï¼Œæ¯”å¦‚åœ¨ Pixel 8 ä¸Šè¿è¡Œã€‚

æ ¹æ® Gemini çš„ [technical report](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)ï¼ŒGemini Pro åŸºæœ¬å¯ä»¥åª²ç¾ GPT-3.5ï¼Œåœ¨ 8 é¡¹è¡Œä¸šæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œæœ‰å…­é¡¹ä¸­è¡¨ç°ä¼˜äº GPT-3.5ï¼Œå…¶ä¸­åŒ…æ‹¬ MMLU åŠ GSM8Kã€‚ Google çš„å®˜æ–¹åšå®¢ [Bard gets its biggest upgrade yet with Gemini](https://blog.google/products/bard/google-bard-try-gemini-ai/) ç§° Gemini Pro å·²ç»é›†æˆåˆ° Bard ä¸Šï¼Œåªä¸è¿‡å½“å‰ä½¿ç”¨çš„æ˜¯ Gemini Pro è‹±æ–‡æ¨¡å‹çš„å¾®è°ƒç‰ˆæœ¬ï¼ŒGemini Poro çš„åŠ å…¥å¢å¼ºäº† Bard çš„æ¨ç†ã€è§„åˆ’ä¸ç†è§£èƒ½åŠ›ã€‚Gemini Ultra ä¼šåœ¨ 2024 å¹´åˆé›†æˆåˆ° Bard Advanced ä¸Šï¼Œæ„Ÿè§‰åº”è¯¥å°±æ˜¯ Google ç‰ˆ ChatGPT premium å§ã€‚ 

Gemini Nano ç»†åˆ†è¿˜æœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼Œä¸€ä¸ª Nano 1, å‚æ•° 1.8Bï¼Œå¦ä¸€ä¸ª Nano 2ï¼Œå‚æ•° 3.25Bã€‚Google åŒæ—¶è¿˜æ”¾å‡ºäº†åŸºäº Gemini çš„ [AlphaCode 2](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf)ï¼Œæ®æŠ¥å‘Šç§°ï¼Œè¾ƒä¸Šä¸€ä»£ AlphaCode å¯ä»¥å¤šè§£å†³ 170% çš„é—®é¢˜ã€‚


# Gemini Pro ä½¿ç”¨ä½“éªŒ 
## è§†é¢‘è§£æèƒ½åŠ›å‡ºä¼—ï¼Œä½†æŒ‡ä»¤å¯¹é½èƒ½åŠ›è¿˜æœ‰å¾…æå‡
ç¬”è€…çº¿ä¸‹æ„Ÿå—äº†ä¸€ä¸‹é›†æˆäº† Gemini Pro çš„æ–°ç‰ˆ Bardï¼Œå®é™…ä½¿ç”¨çš„ä½“éªŒæ˜¯ï¼ŒBard çš„ç†è§£ï¼ˆæ³›åŒ–ï¼‰èƒ½åŠ›è¿˜æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ï¼Œæ‹’ç»å›ç­”é—®é¢˜çš„æƒ…å†µä»ç„¶æ™®éï¼Œå°½ç®¡è¿™ä¸ªé—®é¢˜æ˜¯å®ƒå¯ä»¥è§£å†³çš„ã€‚æ¯”å¦‚ä¸‹é¢æ˜¯ Bard å®˜æ–¹æ¨èçš„ä¸€æ¡åº”ç”¨ç¤ºä¾‹:
```
Give me insights about this video: https://www.youtube.com/watch?v=lr87yrvK86w 
Organize the information in a set of easy to scan bullet points.
```

æ­£å¸¸æƒ…å†µä¸‹ï¼ŒBard ä¼šå¯¹è§†é¢‘è¿›è¡Œåˆ†æï¼Œç„¶åç”Ÿæˆè¿™æ¡è§†é¢‘ç›¸å…³çš„ä¸€äº›è¦ç‚¹ã€‚ä½†å¦‚æœæˆ‘ä»¬æŠŠè§†é¢‘é“¾æ¥æ¢æˆå…¶ä»– YouTube é“¾æ¥ï¼Œæ¯”å¦‚ [Dr. Andrej Karpathy çš„ State of GPT YouTube è§†é¢‘](https://youtu.be/bZQun8Y4L2A?si=8D0NjPDZaLkbXW-f)ï¼Œprompt åé¢å†åŠ ä¸Š reply in Chineseï¼š

```
Give me insights about this video: https://youtube.com/watch?v=bZQun8Y4L2A
Organize the information in a set of easy to scan bullet points and reply in Chinese.
```

Bard å°±ä¸çŸ¥æ‰€æªäº†ï¼Œåªå›ç­”ï¼š
```
I'm not able to help with that, as I'm only a language model.
``` 

äº‹å®ä¸Š Bard æ˜¯æ”¯æŒå›å¤ä¸­æ–‡çš„ï¼Œä¸Šé¢çš„ prompt å¤šæ¬¡å°è¯•åï¼Œæœ‰æœºç‡å¯ä»¥å¾—åˆ°æ­£å¸¸å›å¤ï¼š
```
è§†é¢‘â€œState of GPT | BRK216HFSâ€çš„è§è§£ ...
```

## è®¡ç®—ã€æ¨ç†èƒ½åŠ›å¼ºï¼Œçº é”™èƒ½åŠ›å¼±
ä¹‹å‰æˆ‘ä»¬åœ¨ [Step-Back Prompting]({{site.baseurl}}/2023/12/02/Step-back-prompting/) èŠåˆ°å¯ä»¥é€šè¿‡ step-back prompting æé«˜ LLM çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨ ChatGPT ä¸Šä¹Ÿåšè¿‡æµ‹è¯•ï¼Œæµ‹è¯•æ ·ä¾‹æ˜¯ä¸€é“é«˜ä¸­ç‰©ç†é¢˜

| å¦‚æœæ¸©åº¦å¢åŠ  2 å€ï¼Œä½“ç§¯å¢åŠ  8 å€ï¼Œç†æƒ³æ°”ä½“çš„å‹å¼º $P$ ä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ï¼Ÿ

ChatGPT ç›´æ¥å›ç­”é—®é¢˜æœ‰ä¸€å®šæ¦‚ç‡å‡ºé”™ï¼Œæ¯”å¦‚å‡ºç° Math Errorï¼Œä½†ç»è¿‡æç¤ºä¹‹åå¯ä»¥å°†å‰é¢çš„é”™è¯¯æ”¹æˆè¿‡æ¥ï¼Œä½†æ˜¯ Bard å°±æ¯”è¾ƒè‡ªä¿¡ï¼ˆé¡½å›ºï¼‰ï¼Œæ— è®ºå¦‚ä½•æç¤ºï¼Œéƒ½ä¸ä¼šæ”¹å˜é”™è¯¯çš„ç­”æ¡ˆã€‚ä¸Šé¢˜ä¸­æ­£ç¡®ç»“æœæ˜¯â€œå‹å¼ºé™ä¸ºåŸæ¥çš„ 1/4â€ï¼Œè€Œ Bard ä¸€å£å’¬å®šæ˜¯â€œå‹å¼ºé™ä¸ºåŸæ¥çš„ 1/8 å€â€ï¼š

<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/too_confident_20231208_1109.png" width=769pt>
</figure>


# åŸºå‡†æµ‹è¯•æ•ˆæœ
Gemini æ”¯æŒæ–‡å­—ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘åŒ…æ‹¬è·¨æ¨¡å‹ï¼Œä¸”åœ¨æ¯ä¸ªæ¨¡æ€ä¸Šçš„è¡¨ç°éƒ½å¾ˆå¼ºã€‚Gemini åœ¨ 12 æ–‡æœ¬å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„ 10 é¡¹ï¼Œ9 é¡¹å›¾åƒç†è§£åŸºå‡†æµ‹è¯•ï¼Œ6 é¡¹è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠ 5 é¡¹è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘åŸºå‡†æµ‹è¯•ä¸­éƒ½åˆ·æ–°äº† SoTAã€‚

åœ¨ [Technical Report](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) ä¸­çš„ç¬¬ä¸€ä¸ªä¾‹å­é‡Œï¼ŒGoogle å±•ç¤ºäº† Gemini ä»å›¾ç‰‡ä¸­è¯†åˆ«å­¦ç”Ÿæ½¦è‰çš„å­—ä½“ï¼ŒéªŒè¯å­¦ç”Ÿçš„æ¨ç†è¿‡ç¨‹å¹¶æ ¹æ®è¦æ±‚ç»™å‡ºç¬¦åˆæ ¼å¼çš„è§£ç­”ã€‚è¿™ä¸ªä»»åŠ¡ä½“ç° äº† Gemini å…·å¤‡å¼ºå¤§çš„å›¾ç‰‡è¯†åˆ«ã€æ–‡æœ¬ç†è§£åŠé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚ 

<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/gemini_example_1_20231207_1505.png" width=789pt>
    <figcaption>Example 1: Gemini answers a question about a studentâ€™s handwriting</figcaption>
</figure>

ä¸€äº›æ¨¡å‹çš„å¤šæ¨¡æ€æ˜¯é€šè¿‡è®­ç»ƒå¤šä¸ªä¸åŒçš„æ¨¡æ€æ¨¡å‹å®ç°ï¼Œåœ¨åº”ç”¨ä¸­æ ¹æ®ä¸åŒçš„è¾“å…¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹ï¼Œè€Œ Gemini åœ¨è®­ç»ƒæ—¶å°±æ˜¯é€šè¿‡è·¨æ¨¡æ€è®­ç»ƒçš„ï¼Œè®­ç»ƒæ—¶ä¼šäº¤å‰è¾“å…¥æ–‡æœ¬ã€éŸ³é¢‘åŠè§†è§‰è¾“å…¥ï¼Œåè€…åŒ…æ‹¬å›¾ç‰‡ã€å›¾è¡¨ã€æˆªå›¾ã€PDF åŠè§†é¢‘ï¼Œè¾“å‡ºå¯ä»¥æ˜¯æ–‡æœ¬æˆ–è€…å›¾ç‰‡ã€‚

é‚£ä¹ˆä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œè¿™ç§è”åˆè®­ç»ƒçš„æ–¹å¼å¯¹æ¯”é’ˆå¯¹å•ä¸€é¢†åŸŸä¸“é—¨è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ–¹æ³•ï¼Œå“ªä¸€ä¸ªæ›´å¥½ã€‚Google ç§°å‰è€…æ•ˆæœæ›´å¥½ï¼Œå› ä¸º Gemini ä¸€ç³»åˆ—æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­éƒ½å–å¾—äº† SoTAã€‚

ğŸ¤” è¾¾åˆ°äº† SoTA æ˜¯ä¸é”™ï¼Œç›´è§‚æ„Ÿè§‰ä¹Ÿæ²¡é—®é¢˜ï¼Œä½†æ²¡æœ‰ä½¿ç”¨ç›¸åŒæ•°æ®è®­ç»ƒä¸€ä¸ªå•æ¨¡æ€æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œè¿™ä¸ªç†ç”±æœªå…æœ‰ç‚¹ç‰µå¼ºã€‚


<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/crossmodal_20231208_1122.png" width=778pt>
</figure>

## æ–‡æœ¬
<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/text_benchmark_20231208_1124.png" width=789pt>
    <figcaption>Text benchmarks</figcaption>
</figure>

å¦‚ä¸Šè¡¨æ‰€ç¤ºï¼ŒGemini Ultra åœ¨å‡ é¡¹æ¯”è¾ƒé‡è¦çš„åŸºå‡†æµ‹è¯•è¡¨ç°ä¸­éƒ½æ¯”è¾ƒæŠ¢çœ¼ï¼Œæœ€ä¸ºçªå‡ºçš„æ— ç–‘æ˜¯åœ¨ MMLU ä»»åŠ¡ä¸Šé¦–æ¬¡çªç ´ 90 åˆ†å¤§å…³ï¼Œæ¯”ä¹‹å‰çš„ GPT-4 è¿˜è¦é«˜å‡º 3 ä¸ªç‚¹ã€‚ è¿™ä¸ªçªç ´ä¸»è¦åˆ©ç›Šäº Gemini Ultra ä½¿ç”¨çš„ä¸€ä¸ª *uncertainty-routed chain-of-thought* ç­–ç•¥ï¼Œè¿™ä¸ªç­–ç•¥åœ¨ techinical report å‡ å¥å¸¦è¿‡ï¼Œæ²¡æœ‰è¯¦ç»†è§£é‡Šï¼Œä¹Ÿæ²¡æœ‰ç¤ºä¾‹ã€‚Uncertainty-rounted CoT çš„åŸæ–‡å¦‚ä¸‹ï¼š

```text
...
We proposed a new approach where model produces k chain-of-thought samples, selects the majority vote if the model is confident above a threshold, and otherwise defers to the greedy sample choice. The thresholds are optimized for each model based on their validation split performance. 
...
```

ç¬”è€…æš‚æ—¶è¿˜æ²¡æœ‰å®Œå…¨ç†è§£å…·ä½“æ˜¯å¦‚ä½•æ“ä½œçš„ï¼Œä½†çœ‹èµ·æ¥æœ‰ç§é¢„æµ‹å¤šä¸ªç»“æœç„¶åæŠ•ç¥¨ï¼ˆconsensus votingï¼‰çš„æ„æ€ã€‚NVIDIA çš„ä¸€ä¸ª AI å·¥ç¨‹å¸ˆ [twitter@Sergio Perez](https://x.com/sergiopprz/status/1732502923022684501?s=20) çš„è§è§£ç±»ä¼¼ï¼š

```text
With the "uncertainty-routed" approach, the model generates several answers, each of them with their own CoT. If there's enough consensus among the answers, the model chooses that answer, and if not it reverts to simple maximum-likelihood sampling (i.e. no CoT) at all.
```

è¿™ä¸ªç­–ç•¥çš„æ•ˆæœå®åœ¨æ˜¯è¿‡äºæ˜¾è‘—ï¼Œå› ä¸º Gemini Ultra + 5-shot æ•ˆæœäº‹å®ä¸Šæ¯” GPT-4 + 5-shot ä½ 2.7 ä¸ªç‚¹ï¼Œä½† Gemini Ultra + 32-shot + CoT å†åŠ ä¸Šä¸Šé¢çš„ consensus voting çš„æ–¹æ³•å´åè¶…äº† GPT-4 3 ä¸ªç‚¹ï¼ŒGoogle è¿™æ¬¡ä¸ºäº†åœ¨ MMLU ä¸Šè¶…è¶Š GPT-4 å¯è°“æ˜¯â€œä¸‹è¶³äº†åŠŸå¤«â€ã€‚å› æ­¤æœ‰äººï¼ˆe.g., [twitter@yi_ding](https://twitter.com/yi_ding/status/1732443815804653744)ï¼‰è¡¨ç¤ºè´¹è§£çš„åŒæ—¶ï¼Œä¹Ÿæœ‰æœ‰äººå¯¹ Gemini çš„è®­ç»ƒè¡¨ç¤ºäº†è´¨ç–‘ã€‚ 

<figure style="text-align:center">
    <img src="https://image.ddot.cc/202312/satire_gemini_20231207_2130.png" width=389pt>
    <figcaption>The magic behind Gemini Ultra</figcaption>
</figure>   

å¦å¤–ä¸€ä¸ªæ¯”è¾ƒæ˜¾çœ¼çš„ç»“æœæ˜¯åœ¨ GSM8K ä¸Šè¾¾åˆ°äº† 94.4% çš„å‡†ç¡®ç‡ï¼Œç­–ç•¥æ˜¯ CoT + [self-consistency](https://arxiv.org/pdf/2203.11171.pdf)ï¼Œå°±æ˜¯é¢„æµ‹å¤šä¸ªç»“æœï¼Œç„¶åæŠ•ç¥¨ã€‚æ•ˆæœç¡®å®æ˜¯å¥½äº†ï¼Œä½†è¿™ç§æ–¹å¼è®¡ç®—é‡æœ‰ç‚¹å¤§ã€‚ 

## ç¼–ç¨‹èƒ½åŠ›
Google åŒæ—¶è¿˜æ¨å‡ºåŸºäº Gemini Pro å¾®è°ƒçš„ [AlphaCode 2](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf)ï¼Œ

AlphaCode 2 çš„ä¸»è¦ç»„ä»¶åŒ…æ‹¬ï¼š 
- ç­–ç•¥æ¨¡å‹ï¼ˆpolicy & fine-tuningï¼‰ï¼Œä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆä»£ç ç¤ºä¾‹ï¼› 
- é‡‡æ ·æœºåˆ¶ï¼ˆsamplingï¼‰ï¼Œé¼“åŠ±ç”Ÿæˆå¹¿æ³›å¤šæ ·çš„ä»£ç ç¤ºä¾‹ä»¥æœç´¢å¯èƒ½ç¨‹åºçš„ç©ºé—´ï¼› 
- è¿‡æ»¤æœºåˆ¶ï¼ˆfilteringï¼‰ï¼Œç”¨äºåˆ é™¤ä¸é—®é¢˜æè¿°ä¸ç¬¦çš„ä»£ç ç¤ºä¾‹ï¼› 
- èšç±»ç®—æ³•ï¼ˆclusteringï¼‰ï¼Œå°†è¯­ä¹‰ç›¸ä¼¼çš„ä»£ç ç¤ºä¾‹åˆ†ç»„ï¼Œä»è€Œé¿å…å†—ä½™ï¼› 
- è¯„åˆ†æ¨¡å‹ï¼ˆscoringï¼‰ï¼Œç”¨äºä»æ¯ä¸ªå‰10ä¸ªä»£ç ç¤ºä¾‹é›†ç¾¤ä¸­å‘ˆç°æœ€ä½³å€™é€‰é¡¹ã€‚

æŠ¥å‘Šç§°è¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨ç«æŠ€ç¼–ç¨‹ä¸­è¾¾åˆ° expert æ°´å¹³çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚åœ¨ Codeforces ä¸Šæµ‹è¯•æ—¶ï¼Œç»™ 10 æ¬¡å°è¯•æœºä¼šï¼ŒAlphaCode 2 æœ‰ 43% çš„æ¦‚ç‡ ACï¼Œè€Œä¹‹å‰çš„ AlphaCode åªæœ‰ 25% çš„æ¦‚ç‡ ACã€‚AlphaCode 2 coding èƒ½åŠ›è¿™ä¹ˆå¼ºï¼Œä½†ä¸ºä»€ä¹ˆæ²¡æœ‰æ”¾å‡ºæ¥è®©å¤§å®¶ç”¨ç”¨ï¼ŸGoogle åœ¨ report é‡Œè¯´äº†ï¼šâ€œOur system requires a lot of trial and error, and remains too costly to operate at scale. Further, it relies heavily on being able to filter out obviously bad code samples.â€ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯è¿˜ä¸å¤ªç¨³å®šï¼Œä¸”æˆæœ¬å¤ªé«˜ï¼Œä»ç„¶éœ€è¦ç»§ç»­ä¼˜åŒ–ã€‚å®é™…ä¸Šä»”ç»†è§‚å¯Ÿä¸€ä¸‹ä¸Šé¢çš„ç»„ä»¶ï¼Œå¯ä»¥çœ‹åˆ°å…¶ä¸­åœ¨ policy ä¸ scoring é˜¶æ®µéƒ½éœ€è¦ä¸€ä¸ªæ¨¡å‹ï¼Œè€Œ AlphaCode 2 åˆ™åˆ†åˆ«å¾®è°ƒäº†ä¸€ä¸ª Gemini Pro æ¨¡å‹ï¼Œåœ¨ clustering é˜¶æ®µï¼Œä¹Ÿéœ€è¦å†è®­ç»ƒä¸€ä¸ªæ¨¡å‹è¿›è¡Œèšç±»ã€‚ä¸¤ä¸ª Gemini Pro + ä¸€ä¸ªèšç±»æ¨¡å‹ï¼Œè¿™ä¸ªæˆæœ¬ç¡®å®ä¸ä½ã€‚

# å°ç»“
ä» 2022/11/30 OpenAI å‘å¸ƒ ChatGPT åˆ°ç°åœ¨å·²ç»åˆšå¥½æœ‰ä¸€å¹´çš„æ—¶é—´ï¼Œè¿‡å»çš„ä¸€å¹´å¯¹ Google æ¥è¯´æ˜¯ç–²äºå¥”å‘½çš„ä¸€å¹´ã€‚ç°åœ¨æ˜¯ 2023 çš„æœ€åä¸€ä¸ªæœˆï¼Œçœ¼çœ‹å¿«åˆ°åœ£è¯èŠ‚ï¼ŒGoogle åœ¨è¿™ä¸ªæ—¶é—´ç‚¹æ”¾å‡ºæ¥ Geminiï¼Œä¼°è®¡ä¹Ÿæ˜¯æ€¥äºå¯¹å¤–å±•ç¤ºè‡ªå·±ä»ç„¶æ˜¯ AI é¢†åŸŸçš„ä½¼ä½¼è€…ã€‚Gemini åœ¨åŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸Šçš„è¡¨ç°ç¡®å®å¾ˆå¥½ï¼Œä½†è¦å®é™…æŠ•å…¥ä½¿ç”¨ä¸­è¿˜æœ‰å¾ˆé•¿ä¸€æ®µè·¯è¦èµ°ï¼Œæœ€èµ·ç ç°åœ¨çš„ Bard è¿˜éš¾ä»¥è¾¾åˆ°ç”¨æˆ·çš„è¦æ±‚ã€‚ç­‰åˆ°æ˜å¹´ Ultla æœ€ç»ˆæ”¾å‡ºæ¥æ—¶ï¼ŒOpenAI æˆ–è€… Anthropic Inc. ä¹Ÿè®¸ä¹Ÿä¼šæœ‰æ–°çš„æ¨¡å‹å‘å¸ƒï¼Œåˆ°æ—¶å€™åˆæ˜¯ä¸€åœºæ–°çš„ç«äº‰ã€‚


# å‚è€ƒ 
- AlphaCode 2 technical report: https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf
- Gemini 1.0 technical report: https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf
- Bard gets its biggest upgrade yet with Gemini: https://blog.google/products/bard/google-bard-try-gemini-ai/
- The Best AI Model in the World: Google DeepMindâ€™s Gemini Has Surpassed GPT-4: https://albertoromgar.medium.com/the-best-ai-model-in-the-world-google-deepminds-gemini-has-surpassed-gpt-4-1ee07f84d2ff
- Introducing Gemini: our largest and most capable AI model: https://blog.google/technology/ai/google-gemini-ai/#capabilities

