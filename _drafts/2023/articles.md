---
layout:     post
title:      NLP article reading
date:       2023-04-01
tags: 
categories: 
- nlp,
---


# Reading Progress

1. [TSDAE](https://www.notion.so/slipper/TSDAE-8b98d10d2170492ca42813240ef0c591?pvs=4)
2. [Sentence-Transformer](https://www.notion.so/Sentence-BERT-b5e3e305ae3b47bf8bd72bb7743a7efd)
3. [Word embedding isotraphy](https://www.notion.so/slipper/Isotraphy-42e3b84fed044a47a48f32a544c1aff1?pvs=4)
4. [Attention is all you need]({{site.baseurl}}/2022/09/30/Attention-is-all-you-need-reread/)
5. [Batch normalization]({{site.baseurl}}/2023/10/11/Normalization/)
6. [ALiBi]({{site.baseurl}}/2023/10/27/Attention-With-Linear-Biases/)
7. [ELMo]({{site.baseurl}}/2023/11/06/ElMO/)
8. [Pattern Exploiting Training]({{site.baseurl}}/2023/11/19/Prefix-Tunning/)
9. [SimCSE]({{site.baseurl}}/2022/10/18/Semantic-Similarity/)
10. [How Contextual are Contextualized Word Representations]({{site.baseurl}}/2023/09/28/How-Contextual-are-Contextualized-Word-Representations/)
11. [RAG]({{site.baseurl}}/2023/11/16/Retrivial-augmented-generation/)
12. [Step back prompting]({{site.baseurl}}/2023/12/02/Step-back-prompting/)
13. [Tree of thought]({{site.baseurl}}/2023/12/10/Tree-of-Thoughts/)


## Candidates
- ~~[https://arxliv.org/abs/2104.08821](https://arxiv.org/abs/2104.08821) , SimCSE~~
- https://aclanthology.org/D17-1308.pdf, The strange geometry of skip-gram with negative sampling
- https://arxiv.org/abs/2011.05864 BERT-flow
- ~~https://arxiv.org/pdf/1802.05365.pdf ELMo~~
- [https://kexue.fm/tag/语义相似度/1/](https://kexue.fm/tag/%E8%AF%AD%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6/1/) CoSENT，语义相似度，科学空间
- https://arxiv.org/pdf/1910.10683v3.pdf T5
- https://arxiv.org/pdf/1803.11175.pdf  USE paper
- [https://openreview.net/forum?id=Ov_sMNau-PF](https://openreview.net/pdf?id=Ov_sMNau-PF), ****Semantic Re-tuning with Contrastive Tension,**** Contrast tension
- https://arxiv.org/abs/2011.05864, ****On the Sentence Embeddings from Pre-trained Language Models,**** Bert flow,
- https://aclanthology.org/N16-1162.pdf， Learning Distributed Representations of Sentences from Unlabelled Data，

## Skip Reading List
1. https://aclanthology.org/2020.findings-emnlp.58.pdf MacBERT，pay attention to the dataset they’ve used. 
2. https://arxiv.org/abs/2211.05344, hfl lert model ✅
3. https://arxiv.org/abs/1907.10529 spanBert, a new way to train BERT ✅
4. https://arxiv.org/pdf/2107.02137.pdf , Ernie 3.0 
5. https://arxiv.org/pdf/2309.09558.pdf, summarization is (almost) dead ✅ 
6. https://www.sbert.net/docs/publications.html 
7. https://arxiv.org/pdf/1503.03832.pdf, FaceNet, triplet network
8. [Named Entity Recognition as Dependency Parsing](https://aclanthology.org/2020.acl-main.577.pdf), 2020, NER，biaffine model
9. https://arxiv.org/pdf/2107.13586.pdf, prompt learning survey
10. https://arxiv.org/pdf/2111.01243.pdf, Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey
